{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otg__TfbTiic",
        "outputId": "94588661-8fdd-44b5-f100-ea55f6d5162a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dGuiyjNT9pt",
        "outputId": "9af1e518-1e88-421c-f552-07e9ffd2f4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1DuKt88mVXJTHanb5KcuJgrsmJcTSkPkb/Reproducability Project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/TU Delft/Master/Deep Learning/Reproducability Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejIhtmB3Udi0",
        "outputId": "cfa4b0da-ad03-4861-bdde-e82c227d9db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf_slim) (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tf_slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swEG_s1gUeAW",
        "outputId": "690484e0-bd66-4e77-e667-d6d113a0c2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rawpy\n",
            "  Downloading rawpy-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rawpy) (1.22.4)\n",
            "Installing collected packages: rawpy\n",
            "Successfully installed rawpy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install rawpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg3COytmUPJL",
        "outputId": "b830ddaf-d02d-4a2d-88b2-06dedd1a4da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# uniform content loss + adaptive threshold + per_class_input + recursive G\n",
        "# improvement upon cqf37\n",
        "from __future__ import division\n",
        "import os, time, scipy.io\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tf_slim as slim\n",
        "import numpy as np\n",
        "import rawpy\n",
        "import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syn5U_UjUjXV",
        "outputId": "dd323a61-f2e4-44ff-c398-c8ddf359055d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161\n"
          ]
        }
      ],
      "source": [
        "input_dir = './dataset/Sony/short/'\n",
        "gt_dir = './dataset/Sony/long/'\n",
        "checkpoint_dir = './checkpoint/RMSprop_lr0.1/'\n",
        "result_dir = './result_RMSprop_lr0.1/' \n",
        "\n",
        "# get train IDs\n",
        "train_fns = glob.glob(gt_dir + '0*.ARW')\n",
        "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
        "print(len(train_ids))\n",
        "ps = 512  # patch size for training\n",
        "save_freq = 500\n",
        "num_epochs = 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZMCKs-R9hIa",
        "outputId": "16327f29-e9c5-4b90-e77f-36976398177b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ],
      "source": [
        "#set different debug schemes\n",
        "DEBUG = 2\n",
        "if DEBUG == 1:\n",
        "    save_freq = 2\n",
        "    train_ids = train_ids[0:5]\n",
        "    num_epochs = 10\n",
        "if DEBUG == 2:\n",
        "    save_freq = 5\n",
        "    train_ids = train_ids[0:40]\n",
        "    num_epochs = 15\n",
        "\n",
        "print(len(train_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGWBPgFAU9zu"
      },
      "outputs": [],
      "source": [
        "def lrelu(x):\n",
        "    return tf.maximum(x * 0.2, x)\n",
        "\n",
        "def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n",
        "    \"\"\"\n",
        "    Byte scales an array (image).\n",
        "    Byte scaling means converting the input image to uint8 dtype and scaling\n",
        "    the range to ``(low, high)`` (default 0-255).\n",
        "    If the input image already has dtype uint8, no scaling is done.\n",
        "    This function is only available if Python Imaging Library (PIL) is installed.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        PIL image data array.\n",
        "    cmin : scalar, optional\n",
        "        Bias scaling of small values. Default is ``data.min()``.\n",
        "    cmax : scalar, optional\n",
        "        Bias scaling of large values. Default is ``data.max()``.\n",
        "    high : scalar, optional\n",
        "        Scale max value to `high`.  Default is 255.\n",
        "    low : scalar, optional\n",
        "        Scale min value to `low`.  Default is 0.\n",
        "    Returns\n",
        "    -------\n",
        "    img_array : uint8 ndarray\n",
        "        The byte-scaled array.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from scipy.misc import bytescale\n",
        "    >>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n",
        "    ...                 [ 73.88003259,  80.91433048,   4.88878881],\n",
        "    ...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n",
        "    >>> bytescale(img)\n",
        "    array([[255,   0, 236],\n",
        "           [205, 225,   4],\n",
        "           [140,  90,  70]], dtype=uint8)\n",
        "    >>> bytescale(img, high=200, low=100)\n",
        "    array([[200, 100, 192],\n",
        "           [180, 188, 102],\n",
        "           [155, 135, 128]], dtype=uint8)\n",
        "    >>> bytescale(img, cmin=0, cmax=255)\n",
        "    array([[91,  3, 84],\n",
        "           [74, 81,  5],\n",
        "           [52, 34, 28]], dtype=uint8)\n",
        "    \"\"\"\n",
        "    if data.dtype == np.uint8:\n",
        "        return data\n",
        "\n",
        "    if high > 255:\n",
        "        raise ValueError(\"`high` should be less than or equal to 255.\")\n",
        "    if low < 0:\n",
        "        raise ValueError(\"`low` should be greater than or equal to 0.\")\n",
        "    if high < low:\n",
        "        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n",
        "\n",
        "    if cmin is None:\n",
        "        cmin = data.min()\n",
        "    if cmax is None:\n",
        "        cmax = data.max()\n",
        "\n",
        "    cscale = cmax - cmin\n",
        "    if cscale < 0:\n",
        "        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n",
        "    elif cscale == 0:\n",
        "        cscale = 1\n",
        "\n",
        "    scale = float(high - low) / cscale\n",
        "    bytedata = (data - cmin) * scale + low\n",
        "    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "def toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n",
        "            mode=None, channel_axis=None):\n",
        "    \"\"\"Takes a numpy array and returns a PIL image.\n",
        "    This function is only available if Python Imaging Library (PIL) is installed.\n",
        "    The mode of the PIL image depends on the array shape and the `pal` and\n",
        "    `mode` keywords.\n",
        "    For 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n",
        "    (from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\n",
        "    is given as 'F' or 'I' in which case a float and/or integer array is made.\n",
        "    .. warning::\n",
        "        This function uses `bytescale` under the hood to rescale images to use\n",
        "        the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n",
        "        It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n",
        "        (which is the default).\n",
        "    Notes\n",
        "    -----\n",
        "    For 3-D arrays, the `channel_axis` argument tells which dimension of the\n",
        "    array holds the channel data.\n",
        "    For 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\n",
        "    by default or 'YCbCr' if selected.\n",
        "    The numpy array must be either 2 dimensional or 3 dimensional.\n",
        "    \"\"\"\n",
        "    data = np.asarray(arr)\n",
        "    if np.iscomplexobj(data):\n",
        "        raise ValueError(\"Cannot convert a complex-valued array.\")\n",
        "    shape = list(data.shape)\n",
        "    valid = len(shape) == 2 or ((len(shape) == 3) and\n",
        "                                ((3 in shape) or (4 in shape)))\n",
        "    if not valid:\n",
        "        raise ValueError(\"'arr' does not have a suitable array shape for \"\n",
        "                         \"any mode.\")\n",
        "    if len(shape) == 2:\n",
        "        shape = (shape[1], shape[0])  # columns show up first\n",
        "        if mode == 'F':\n",
        "            data32 = data.astype(np.float32)\n",
        "            image = Image.frombytes(mode, shape, data32.tostring())\n",
        "            return image\n",
        "        if mode in [None, 'L', 'P']:\n",
        "            bytedata = bytescale(data, high=high, low=low,\n",
        "                                 cmin=cmin, cmax=cmax)\n",
        "            image = Image.frombytes('L', shape, bytedata.tostring())\n",
        "            if pal is not None:\n",
        "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
        "                # Becomes a mode='P' automagically.\n",
        "            elif mode == 'P':  # default gray-scale\n",
        "                pal = (np.arange(0, 256, 1, dtype=np.uint8)[:, np.newaxis] *\n",
        "                       np.ones((3,), dtype=np.uint8)[np.newaxis, :])\n",
        "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
        "            return image\n",
        "        if mode == '1':  # high input gives threshold for 1\n",
        "            bytedata = (data > high)\n",
        "            image = Image.frombytes('1', shape, bytedata.tostring())\n",
        "            return image\n",
        "        if cmin is None:\n",
        "            cmin = np.amin(np.ravel(data))\n",
        "        if cmax is None:\n",
        "            cmax = np.amax(np.ravel(data))\n",
        "        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n",
        "        if mode == 'I':\n",
        "            data32 = data.astype(np.uint32)\n",
        "            image = Image.frombytes(mode, shape, data32.tostring())\n",
        "        else:\n",
        "            raise ValueError(_errstr)\n",
        "        return image\n",
        "\n",
        "    # if here then 3-d array with a 3 or a 4 in the shape length.\n",
        "    # Check for 3 in datacube shape --- 'RGB' or 'YCbCr'\n",
        "    if channel_axis is None:\n",
        "        if (3 in shape):\n",
        "            ca = np.flatnonzero(np.asarray(shape) == 3)[0]\n",
        "        else:\n",
        "            ca = np.flatnonzero(np.asarray(shape) == 4)\n",
        "            if len(ca):\n",
        "                ca = ca[0]\n",
        "            else:\n",
        "                raise ValueError(\"Could not find channel dimension.\")\n",
        "    else:\n",
        "        ca = channel_axis\n",
        "\n",
        "    numch = shape[ca]\n",
        "    if numch not in [3, 4]:\n",
        "        raise ValueError(\"Channel axis dimension is not valid.\")\n",
        "\n",
        "    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n",
        "    if ca == 2:\n",
        "        strdata = bytedata.tostring()\n",
        "        shape = (shape[1], shape[0])\n",
        "    elif ca == 1:\n",
        "        strdata = np.transpose(bytedata, (0, 2, 1)).tostring()\n",
        "        shape = (shape[2], shape[0])\n",
        "    elif ca == 0:\n",
        "        strdata = np.transpose(bytedata, (1, 2, 0)).tostring()\n",
        "        shape = (shape[2], shape[1])\n",
        "    if mode is None:\n",
        "        if numch == 3:\n",
        "            mode = 'RGB'\n",
        "        else:\n",
        "            mode = 'RGBA'\n",
        "\n",
        "    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n",
        "        raise ValueError(_errstr)\n",
        "\n",
        "    if mode in ['RGB', 'YCbCr']:\n",
        "        if numch != 3:\n",
        "            raise ValueError(\"Invalid array shape for mode.\")\n",
        "    if mode in ['RGBA', 'CMYK']:\n",
        "        if numch != 4:\n",
        "            raise ValueError(\"Invalid array shape for mode.\")\n",
        "\n",
        "    # Here we know data and mode is correct\n",
        "    image = Image.frombytes(mode, shape, strdata)\n",
        "    return image\n",
        "\n",
        "def upsample_and_concat(x1, x2, output_channels, in_channels):\n",
        "    pool_size = 2\n",
        "    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n",
        "    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n",
        "\n",
        "    deconv_output = tf.concat([deconv, x2], 3)\n",
        "    deconv_output.set_shape([None, None, None, output_channels * 2])\n",
        "\n",
        "    return deconv_output\n",
        "\n",
        "\n",
        "def network(input):\n",
        "    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
        "    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
        "    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
        "\n",
        "    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
        "    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
        "    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
        "\n",
        "    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
        "    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
        "    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
        "\n",
        "    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
        "    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
        "    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
        "\n",
        "    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
        "    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
        "\n",
        "    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
        "    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
        "    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
        "\n",
        "    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
        "    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
        "    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
        "\n",
        "    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
        "    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
        "    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
        "\n",
        "    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
        "    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
        "    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
        "\n",
        "    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
        "    out = tf.depth_to_space(conv10, 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    black = raw.black_level_per_channel\n",
        "    im = np.maximum((im - np.mean(black)), 0) / ((16383) - np.mean(black))  # subtract the black level\n",
        "  \n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "  \n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "    if (raw.raw_pattern == ([[1, 0], [2, 3]])).all():\n",
        "      G = im[0:H:2, 0:W:2, :] # Every alternating value starting from position (0,0) \n",
        "      R = im[0:H:2, 1:W:2, :] # Every alternating value starting from position (0,1) \n",
        "      G_e = im[1:H:2, 1:W:2, :] # Every alternating value starting from position (1,1) \n",
        "      B = im[1:H:2, 0:W:2, :] # Every alternating value starting from position (1,0)\n",
        "    if (raw.raw_pattern == ([[0, 1], [3, 2]])).all():\n",
        "      R = im[0:H:2, 0:W:2, :] # Every alternating value starting from position (0,0) \n",
        "      G = im[0:H:2, 1:W:2, :] # Every alternating value starting from position (0,1) \n",
        "      B = im[1:H:2, 1:W:2, :] # Every alternating value starting from position (1,1) \n",
        "      G_e = im[1:H:2, 0:W:2, :] # Every alternating value starting from position (1,0)  \n",
        "    if (raw.raw_pattern == ([[1, 2], [0, 3]])).all():\n",
        "      G = im[0:H:2, 0:W:2, :] # Every alternating value starting from position (0,0) \n",
        "      B = im[0:H:2, 1:W:2, :] # Every alternating value starting from position (0,1) \n",
        "      G_e = im[1:H:2, 1:W:2, :] # Every alternating value starting from position (1,1) \n",
        "      R = im[1:H:2, 0:W:2, :] # Every alternating value starting from position (1,0)\n",
        "    if (raw.raw_pattern == ([[2, 1], [3, 0]])).all():\n",
        "      B = im[0:H:2, 0:W:2, :] # Every alternating value starting from position (0,0) \n",
        "      G = im[0:H:2, 1:W:2, :] # Every alternating value starting from position (0,1) \n",
        "      R = im[1:H:2, 1:W:2, :] # Every alternating value starting from position (1,1) \n",
        "      G_e = im[1:H:2, 0:W:2, :] # Every alternating value starting from position (1,0)\n",
        "    out = np.concatenate((R, G, B, G_e), axis=2) # Always in R-G-B-G format\n",
        "    \n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIet0qLUZNOI",
        "outputId": "9f949680-6037-4606-a14e-ded3873254e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "sess = tf.Session()\n",
        "in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n",
        "gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "out_image = network(in_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABWciBIgVWx5",
        "outputId": "3dd42c45-552c-4c82-db59-2a4c217228a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug scheme:  2 Nr. images:  40\n",
            "110\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fa6d1088910b>:49: DeprecationWarning: This function is deprecated. Please call randint(0, 0 + 1) instead\n",
            "  in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 Loss=0.089 Time=21.345\n",
            "72"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-ef9c8e6e316a>:157: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  strdata = bytedata.tostring()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1\n",
            "0 2 Loss=0.250 Time=19.154\n",
            "99\n",
            "1\n",
            "0 3 Loss=0.209 Time=66.525\n",
            "85\n",
            "1\n",
            "0 4 Loss=0.215 Time=14.877\n",
            "96\n",
            "1\n",
            "0 5 Loss=0.213 Time=15.214\n",
            "119\n",
            "1\n",
            "0 6 Loss=0.199 Time=14.905\n",
            "113\n",
            "1\n",
            "0 7 Loss=0.187 Time=14.586\n",
            "114\n",
            "1\n",
            "0 8 Loss=0.168 Time=14.675\n",
            "130\n",
            "1\n",
            "0 9 Loss=0.168 Time=16.223\n",
            "81\n",
            "1\n",
            "0 10 Loss=0.180 Time=15.165\n",
            "84\n",
            "1\n",
            "0 11 Loss=0.192 Time=14.347\n",
            "76\n",
            "1\n",
            "0 12 Loss=0.188 Time=14.342\n",
            "98\n",
            "1\n",
            "0 13 Loss=0.195 Time=14.278\n",
            "88\n",
            "1\n",
            "0 14 Loss=0.192 Time=15.369\n",
            "127\n",
            "1\n",
            "0 15 Loss=0.186 Time=15.852\n",
            "117\n",
            "1\n",
            "0 16 Loss=0.185 Time=14.931\n",
            "128\n",
            "1\n",
            "0 17 Loss=0.180 Time=14.087\n",
            "122\n",
            "1\n",
            "0 18 Loss=0.173 Time=14.996\n",
            "95\n",
            "1\n",
            "0 19 Loss=0.171 Time=14.926\n",
            "121\n",
            "1\n",
            "0 20 Loss=0.167 Time=14.853\n",
            "104\n",
            "1\n",
            "0 21 Loss=0.164 Time=15.037\n",
            "73\n",
            "1\n",
            "0 22 Loss=0.163 Time=15.006\n",
            "123\n",
            "1\n",
            "0 23 Loss=0.164 Time=15.015\n",
            "124\n",
            "1\n",
            "0 24 Loss=0.166 Time=15.303\n",
            "90\n",
            "1\n",
            "0 25 Loss=0.166 Time=19.845\n",
            "83\n",
            "1\n",
            "0 26 Loss=0.165 Time=16.074\n",
            "92\n",
            "1\n",
            "0 27 Loss=0.171 Time=16.043\n",
            "75\n",
            "1\n",
            "0 28 Loss=0.174 Time=15.440\n",
            "108\n",
            "1\n",
            "0 29 Loss=0.172 Time=15.319\n",
            "109\n",
            "1\n",
            "0 30 Loss=0.168 Time=15.246\n",
            "78\n",
            "1\n",
            "0 31 Loss=0.176 Time=15.478\n",
            "112\n",
            "1\n",
            "0 32 Loss=0.175 Time=15.150\n",
            "102\n",
            "1\n",
            "0 33 Loss=0.174 Time=15.054\n",
            "118\n",
            "1\n",
            "0 34 Loss=0.172 Time=14.709\n",
            "94\n",
            "1\n",
            "0 35 Loss=0.169 Time=14.509\n",
            "100\n",
            "1\n",
            "0 36 Loss=0.166 Time=15.680\n",
            "86\n",
            "1\n",
            "0 37 Loss=0.163 Time=15.544\n",
            "97\n",
            "1\n",
            "0 38 Loss=0.167 Time=15.347\n",
            "91\n",
            "1\n",
            "0 39 Loss=0.166 Time=15.248\n",
            "129\n",
            "1\n",
            "0 40 Loss=0.166 Time=15.546\n",
            "96\n",
            "1\n",
            "1 1 Loss=0.170 Time=12.547\n",
            "90\n",
            "1\n",
            "1 2 Loss=0.170 Time=10.207\n",
            "75\n",
            "1\n",
            "1 3 Loss=0.167 Time=12.393\n",
            "97\n",
            "1\n",
            "1 4 Loss=0.165 Time=12.379\n",
            "118\n",
            "1\n",
            "1 5 Loss=0.168 Time=12.333\n",
            "99\n",
            "1\n",
            "1 6 Loss=0.168 Time=11.899\n",
            "124\n",
            "1\n",
            "1 7 Loss=0.169 Time=10.618\n",
            "76\n",
            "1\n",
            "1 8 Loss=0.167 Time=12.368\n",
            "86\n",
            "1\n",
            "1 9 Loss=0.170 Time=12.373\n",
            "117\n",
            "1\n",
            "1 10 Loss=0.171 Time=12.429\n",
            "78\n",
            "1\n",
            "1 11 Loss=0.164 Time=11.734\n",
            "122\n",
            "1\n",
            "1 12 Loss=0.165 Time=10.919\n",
            "72\n",
            "1\n",
            "1 13 Loss=0.165 Time=12.353\n",
            "112\n",
            "1\n",
            "1 14 Loss=0.169 Time=12.372\n",
            "108\n",
            "1\n",
            "1 15 Loss=0.174 Time=12.336\n",
            "130\n",
            "1\n",
            "1 16 Loss=0.172 Time=11.355\n",
            "84\n",
            "1\n",
            "1 17 Loss=0.174 Time=11.238\n",
            "123\n",
            "1\n",
            "1 18 Loss=0.176 Time=12.427\n",
            "104\n",
            "1\n",
            "1 19 Loss=0.177 Time=12.378\n",
            "128\n",
            "1\n",
            "1 20 Loss=0.179 Time=12.319\n",
            "95\n",
            "1\n",
            "1 21 Loss=0.177 Time=11.129\n",
            "102\n",
            "1\n",
            "1 22 Loss=0.179 Time=11.542\n",
            "98\n",
            "1\n",
            "1 23 Loss=0.179 Time=12.255\n",
            "127\n",
            "1\n",
            "1 24 Loss=0.179 Time=12.299\n",
            "110\n",
            "1\n",
            "1 25 Loss=0.181 Time=12.249\n",
            "114\n",
            "1\n",
            "1 26 Loss=0.183 Time=10.273\n",
            "91\n",
            "1\n",
            "1 27 Loss=0.183 Time=12.090\n",
            "119\n",
            "1\n",
            "1 28 Loss=0.182 Time=12.345\n",
            "81\n",
            "1\n",
            "1 29 Loss=0.178 Time=12.300\n",
            "85\n",
            "1\n",
            "1 30 Loss=0.177 Time=11.830\n",
            "94\n",
            "1\n",
            "1 31 Loss=0.180 Time=10.482\n",
            "83\n",
            "1\n",
            "1 32 Loss=0.191 Time=12.305\n",
            "113\n",
            "1\n",
            "1 33 Loss=0.193 Time=12.292\n",
            "100\n",
            "1\n",
            "1 34 Loss=0.194 Time=12.246\n",
            "121\n",
            "1\n",
            "1 35 Loss=0.194 Time=10.953\n",
            "129\n",
            "1\n",
            "1 36 Loss=0.197 Time=11.401\n",
            "88\n",
            "1\n",
            "1 37 Loss=0.197 Time=12.260\n",
            "92\n",
            "1\n",
            "1 38 Loss=0.190 Time=12.311\n",
            "73\n",
            "1\n",
            "1 39 Loss=0.188 Time=12.279\n",
            "109\n",
            "1\n",
            "1 40 Loss=0.188 Time=10.406\n",
            "73\n",
            "1\n",
            "2 1 Loss=0.189 Time=12.031\n",
            "97\n",
            "1\n",
            "2 2 Loss=0.189 Time=12.285\n",
            "109\n",
            "1\n",
            "2 3 Loss=0.193 Time=12.239\n",
            "119\n",
            "1\n",
            "2 4 Loss=0.194 Time=12.192\n",
            "99\n",
            "1\n",
            "2 5 Loss=0.192 Time=10.222\n",
            "83\n",
            "1\n",
            "2 6 Loss=0.189 Time=12.147\n",
            "127\n",
            "1\n",
            "2 7 Loss=27.931 Time=12.346\n",
            "129\n",
            "1\n",
            "2 8 Loss=1278641487.124 Time=12.312\n",
            "72\n",
            "1\n",
            "2 9 Loss=8495330463030541829076418560.000 Time=11.871\n",
            "85\n",
            "1\n",
            "2 10 Loss=11520931070784218948756832256.000 Time=11.205\n",
            "78\n",
            "1\n",
            "2 11 Loss=nan Time=12.096\n",
            "112\n",
            "1\n",
            "2 12 Loss=nan Time=12.207\n",
            "76\n",
            "1\n",
            "2 13 Loss=nan Time=12.228\n",
            "88\n",
            "1\n",
            "2 14 Loss=nan Time=11.333\n",
            "123\n",
            "1\n",
            "2 15 Loss=nan Time=10.873\n",
            "117\n",
            "1\n",
            "2 16 Loss=nan Time=12.195\n",
            "130\n",
            "1\n",
            "2 17 Loss=nan Time=12.211\n",
            "108\n",
            "1\n",
            "2 18 Loss=nan Time=12.154\n",
            "95\n",
            "1\n",
            "2 19 Loss=nan Time=10.287\n",
            "86\n",
            "1\n",
            "2 20 Loss=nan Time=11.901\n",
            "124\n",
            "1\n",
            "2 21 Loss=nan Time=12.312\n",
            "128\n",
            "1\n",
            "2 22 Loss=nan Time=12.232\n",
            "75\n",
            "1\n",
            "2 23 Loss=nan Time=11.499\n",
            "121\n",
            "1\n",
            "2 24 Loss=nan Time=10.755\n",
            "104\n",
            "1\n",
            "2 25 Loss=nan Time=12.191\n",
            "98\n",
            "1\n",
            "2 26 Loss=nan Time=12.208\n",
            "94\n",
            "1\n",
            "2 27 Loss=nan Time=12.234\n",
            "122\n",
            "1\n",
            "2 28 Loss=nan Time=10.768\n",
            "90\n",
            "1\n",
            "2 29 Loss=nan Time=11.714\n",
            "92\n",
            "1\n",
            "2 30 Loss=nan Time=12.335\n",
            "100\n",
            "1\n",
            "2 31 Loss=nan Time=12.252\n",
            "102\n",
            "1\n",
            "2 32 Loss=nan Time=12.387\n",
            "110\n",
            "1\n",
            "2 33 Loss=nan Time=10.459\n",
            "118\n",
            "1\n",
            "2 34 Loss=nan Time=12.036\n",
            "91\n",
            "1\n",
            "2 35 Loss=nan Time=12.578\n",
            "113\n",
            "1\n",
            "2 36 Loss=nan Time=12.344\n",
            "114\n",
            "1\n",
            "2 37 Loss=nan Time=12.320\n",
            "96\n",
            "1\n",
            "2 38 Loss=nan Time=10.178\n",
            "84\n",
            "1\n",
            "2 39 Loss=nan Time=12.386\n",
            "81\n",
            "1\n",
            "2 40 Loss=nan Time=12.411\n",
            "97\n",
            "1\n",
            "3 1 Loss=nan Time=12.771\n",
            "75\n",
            "1\n",
            "3 2 Loss=nan Time=12.323\n",
            "85\n",
            "1\n",
            "3 3 Loss=nan Time=10.372\n",
            "113\n",
            "1\n",
            "3 4 Loss=nan Time=12.025\n",
            "117\n",
            "1\n",
            "3 5 Loss=nan Time=12.222\n",
            "86\n",
            "1\n",
            "3 6 Loss=nan Time=12.211\n",
            "92\n",
            "1\n",
            "3 7 Loss=nan Time=11.733\n",
            "129\n",
            "1\n",
            "3 8 Loss=nan Time=10.698\n",
            "112\n",
            "1\n",
            "3 9 Loss=nan Time=12.241\n",
            "110\n",
            "1\n",
            "3 10 Loss=nan Time=12.274\n",
            "122\n",
            "1\n",
            "3 11 Loss=nan Time=12.170\n",
            "91\n",
            "1\n",
            "3 12 Loss=nan Time=11.197\n",
            "109\n",
            "1\n",
            "3 13 Loss=nan Time=11.165\n",
            "99\n",
            "1\n",
            "3 14 Loss=nan Time=12.223\n",
            "95\n",
            "1\n",
            "3 15 Loss=nan Time=12.302\n",
            "90\n",
            "1\n",
            "3 16 Loss=nan Time=12.153\n",
            "81\n",
            "1\n",
            "3 17 Loss=nan Time=10.527\n",
            "123\n",
            "1\n",
            "3 18 Loss=nan Time=11.770\n",
            "83\n",
            "1\n",
            "3 19 Loss=nan Time=12.126\n",
            "78\n",
            "1\n",
            "3 20 Loss=nan Time=12.187\n",
            "104\n",
            "1\n",
            "3 21 Loss=nan Time=11.757\n",
            "121\n",
            "1\n",
            "3 22 Loss=nan Time=10.434\n",
            "114\n",
            "1\n",
            "3 23 Loss=nan Time=12.223\n",
            "119\n",
            "1\n",
            "3 24 Loss=nan Time=12.177\n",
            "72\n",
            "1\n",
            "3 25 Loss=nan Time=12.225\n",
            "130\n",
            "1\n",
            "3 26 Loss=nan Time=10.988\n",
            "118\n",
            "1\n",
            "3 27 Loss=nan Time=11.205\n",
            "76\n",
            "1\n",
            "3 28 Loss=nan Time=12.245\n",
            "124\n",
            "1\n",
            "3 29 Loss=nan Time=12.355\n",
            "88\n",
            "1\n",
            "3 30 Loss=nan Time=12.250\n",
            "98\n",
            "1\n",
            "3 31 Loss=nan Time=11.236\n",
            "84\n",
            "1\n",
            "3 32 Loss=nan Time=11.611\n",
            "102\n",
            "1\n",
            "3 33 Loss=nan Time=12.282\n",
            "108\n",
            "1\n",
            "3 34 Loss=nan Time=12.238\n",
            "128\n",
            "1\n",
            "3 35 Loss=nan Time=12.370\n",
            "127\n",
            "1\n",
            "3 36 Loss=nan Time=10.416\n",
            "73\n",
            "1\n",
            "3 37 Loss=nan Time=12.098\n",
            "96\n",
            "1\n",
            "3 38 Loss=nan Time=12.290\n",
            "94\n",
            "1\n",
            "3 39 Loss=nan Time=12.243\n",
            "100\n",
            "1\n",
            "3 40 Loss=nan Time=12.270\n",
            "91\n",
            "1\n",
            "4 1 Loss=nan Time=10.582\n",
            "102\n",
            "1\n",
            "4 2 Loss=nan Time=12.205\n",
            "122\n",
            "1\n",
            "4 3 Loss=nan Time=12.294\n",
            "92\n",
            "1\n",
            "4 4 Loss=nan Time=12.207\n",
            "117\n",
            "1\n",
            "4 5 Loss=nan Time=11.677\n",
            "90\n",
            "1\n",
            "4 6 Loss=nan Time=10.501\n",
            "127\n",
            "1\n",
            "4 7 Loss=nan Time=12.293\n",
            "98\n",
            "1\n",
            "4 8 Loss=nan Time=12.338\n",
            "75\n",
            "1\n",
            "4 9 Loss=nan Time=12.376\n",
            "96\n",
            "1\n",
            "4 10 Loss=nan Time=11.683\n",
            "72\n",
            "1\n",
            "4 11 Loss=nan Time=10.860\n",
            "94\n",
            "1\n",
            "4 12 Loss=nan Time=12.211\n",
            "100\n",
            "1\n",
            "4 13 Loss=nan Time=12.351\n",
            "119\n",
            "1\n",
            "4 14 Loss=nan Time=12.289\n",
            "88\n",
            "1\n",
            "4 15 Loss=nan Time=10.846\n",
            "130\n",
            "1\n",
            "4 16 Loss=nan Time=11.572\n",
            "113\n",
            "1\n",
            "4 17 Loss=nan Time=12.190\n",
            "112\n",
            "1\n",
            "4 18 Loss=nan Time=12.298\n",
            "129\n",
            "1\n",
            "4 19 Loss=nan Time=12.101\n",
            "109\n",
            "1\n",
            "4 20 Loss=nan Time=10.181\n",
            "118\n",
            "1\n",
            "4 21 Loss=nan Time=12.337\n",
            "104\n",
            "1\n",
            "4 22 Loss=nan Time=12.358\n",
            "124\n",
            "1\n",
            "4 23 Loss=nan Time=12.310\n",
            "73\n",
            "1\n",
            "4 24 Loss=nan Time=11.344\n",
            "81\n",
            "1\n",
            "4 25 Loss=nan Time=10.942\n",
            "128\n",
            "1\n",
            "4 26 Loss=nan Time=12.212\n",
            "76\n",
            "1\n",
            "4 27 Loss=nan Time=12.339\n",
            "84\n",
            "1\n",
            "4 28 Loss=nan Time=12.304\n",
            "85\n",
            "1\n",
            "4 29 Loss=nan Time=11.314\n",
            "95\n",
            "1\n",
            "4 30 Loss=nan Time=11.211\n",
            "83\n",
            "1\n",
            "4 31 Loss=nan Time=12.465\n",
            "114\n",
            "1\n",
            "4 32 Loss=nan Time=12.299\n",
            "121\n",
            "1\n",
            "4 33 Loss=nan Time=12.307\n",
            "123\n",
            "1\n",
            "4 34 Loss=nan Time=10.423\n",
            "99\n",
            "1\n",
            "4 35 Loss=nan Time=11.888\n",
            "97\n",
            "1\n",
            "4 36 Loss=nan Time=12.280\n",
            "108\n",
            "1\n",
            "4 37 Loss=nan Time=12.351\n",
            "110\n",
            "1\n",
            "4 38 Loss=nan Time=12.296\n",
            "78\n",
            "1\n",
            "4 39 Loss=nan Time=10.277\n",
            "86\n",
            "1\n",
            "4 40 Loss=nan Time=12.109\n",
            "72\n",
            "1\n",
            "5 1 Loss=nan Time=12.638\n",
            "128\n",
            "1\n",
            "5 2 Loss=nan Time=12.209\n",
            "124\n",
            "1\n",
            "5 3 Loss=nan Time=12.321\n",
            "95\n",
            "1\n",
            "5 4 Loss=nan Time=10.882\n",
            "83\n",
            "1\n",
            "5 5 Loss=nan Time=11.407\n",
            "75\n",
            "1\n",
            "5 6 Loss=nan Time=12.250\n",
            "73\n",
            "1\n",
            "5 7 Loss=nan Time=12.320\n",
            "114\n",
            "1\n",
            "5 8 Loss=nan Time=12.553\n",
            "76\n",
            "1\n",
            "5 9 Loss=nan Time=10.889\n",
            "86\n",
            "1\n",
            "5 10 Loss=nan Time=11.434\n",
            "121\n",
            "1\n",
            "5 11 Loss=nan Time=12.451\n",
            "118\n",
            "1\n",
            "5 12 Loss=nan Time=12.425\n",
            "113\n",
            "1\n",
            "5 13 Loss=nan Time=12.496\n",
            "123\n",
            "1\n",
            "5 14 Loss=nan Time=10.982\n",
            "92\n",
            "1\n",
            "5 15 Loss=nan Time=11.370\n",
            "102\n",
            "1\n",
            "5 16 Loss=nan Time=12.280\n",
            "90\n",
            "1\n",
            "5 17 Loss=nan Time=12.518\n",
            "127\n",
            "1\n",
            "5 18 Loss=nan Time=12.312\n",
            "100\n",
            "1\n",
            "5 19 Loss=nan Time=11.246\n",
            "110\n",
            "1\n",
            "5 20 Loss=nan Time=11.187\n",
            "88\n",
            "1\n",
            "5 21 Loss=nan Time=12.394\n",
            "129\n",
            "1\n",
            "5 22 Loss=nan Time=12.399\n",
            "108\n",
            "1\n",
            "5 23 Loss=nan Time=12.636\n",
            "99\n",
            "1\n",
            "5 24 Loss=nan Time=11.661\n",
            "81\n",
            "1\n",
            "5 25 Loss=nan Time=10.701\n",
            "98\n",
            "1\n",
            "5 26 Loss=nan Time=12.333\n",
            "85\n",
            "1\n",
            "5 27 Loss=nan Time=12.310\n",
            "122\n",
            "1\n",
            "5 28 Loss=nan Time=12.255\n",
            "97\n",
            "1\n",
            "5 29 Loss=nan Time=10.996\n",
            "104\n",
            "1\n",
            "5 30 Loss=nan Time=11.399\n",
            "117\n",
            "1\n",
            "5 31 Loss=nan Time=12.448\n",
            "119\n",
            "1\n",
            "5 32 Loss=nan Time=12.438\n",
            "96\n",
            "1\n",
            "5 33 Loss=nan Time=12.341\n",
            "112\n",
            "1\n",
            "5 34 Loss=nan Time=11.505\n",
            "91\n",
            "1\n",
            "5 35 Loss=nan Time=10.770\n",
            "130\n",
            "1\n",
            "5 36 Loss=nan Time=12.295\n",
            "94\n",
            "1\n",
            "5 37 Loss=nan Time=12.308\n",
            "109\n",
            "1\n",
            "5 38 Loss=nan Time=12.302\n",
            "84\n",
            "1\n",
            "5 39 Loss=nan Time=11.538\n",
            "78\n",
            "1\n",
            "5 40 Loss=nan Time=10.756\n",
            "130\n",
            "1\n",
            "6 1 Loss=nan Time=12.631\n",
            "84\n",
            "1\n",
            "6 2 Loss=nan Time=12.285\n",
            "118\n",
            "1\n",
            "6 3 Loss=nan Time=12.206\n",
            "98\n",
            "1\n",
            "6 4 Loss=nan Time=11.646\n",
            "119\n",
            "1\n",
            "6 5 Loss=nan Time=10.359\n",
            "108\n",
            "1\n",
            "6 6 Loss=nan Time=12.167\n",
            "94\n",
            "1\n",
            "6 7 Loss=nan Time=12.344\n",
            "113\n",
            "1\n",
            "6 8 Loss=nan Time=12.193\n",
            "124\n",
            "1\n",
            "6 9 Loss=nan Time=10.932\n",
            "73\n",
            "1\n",
            "6 10 Loss=nan Time=11.463\n",
            "123\n",
            "1\n",
            "6 11 Loss=nan Time=12.295\n",
            "110\n",
            "1\n",
            "6 12 Loss=nan Time=12.259\n",
            "128\n",
            "1\n",
            "6 13 Loss=nan Time=12.187\n",
            "72\n",
            "1\n",
            "6 14 Loss=nan Time=10.056\n",
            "96\n",
            "1\n",
            "6 15 Loss=nan Time=12.274\n",
            "85\n",
            "1\n",
            "6 16 Loss=nan Time=12.255\n",
            "76\n",
            "1\n",
            "6 17 Loss=nan Time=12.235\n",
            "88\n",
            "1\n",
            "6 18 Loss=nan Time=11.528\n",
            "129\n",
            "1\n",
            "6 19 Loss=nan Time=10.744\n",
            "104\n",
            "1\n",
            "6 20 Loss=nan Time=12.278\n",
            "102\n",
            "1\n",
            "6 21 Loss=nan Time=12.349\n",
            "122\n",
            "1\n",
            "6 22 Loss=nan Time=12.209\n",
            "100\n",
            "1\n",
            "6 23 Loss=nan Time=10.667\n",
            "114\n",
            "1\n",
            "6 24 Loss=nan Time=11.768\n",
            "117\n",
            "1\n",
            "6 25 Loss=nan Time=12.188\n",
            "81\n",
            "1\n",
            "6 26 Loss=nan Time=12.240\n",
            "112\n",
            "1\n",
            "6 27 Loss=nan Time=11.993\n",
            "78\n",
            "1\n",
            "6 28 Loss=nan Time=10.234\n",
            "90\n",
            "1\n",
            "6 29 Loss=nan Time=12.317\n",
            "99\n",
            "1\n",
            "6 30 Loss=nan Time=12.259\n",
            "83\n",
            "1\n",
            "6 31 Loss=nan Time=12.258\n",
            "109\n",
            "1\n",
            "6 32 Loss=nan Time=11.792\n",
            "91\n",
            "1\n",
            "6 33 Loss=nan Time=10.796\n",
            "95\n",
            "1\n",
            "6 34 Loss=nan Time=12.124\n",
            "97\n",
            "1\n",
            "6 35 Loss=nan Time=12.243\n",
            "86\n",
            "1\n",
            "6 36 Loss=nan Time=12.212\n",
            "75\n",
            "1\n",
            "6 37 Loss=nan Time=10.947\n",
            "127\n",
            "1\n",
            "6 38 Loss=nan Time=11.479\n",
            "92\n",
            "1\n",
            "6 39 Loss=nan Time=12.181\n",
            "121\n",
            "1\n",
            "6 40 Loss=nan Time=12.184\n",
            "78\n",
            "1\n",
            "7 1 Loss=nan Time=12.559\n",
            "110\n",
            "1\n",
            "7 2 Loss=nan Time=10.733\n",
            "97\n",
            "1\n",
            "7 3 Loss=nan Time=11.613\n",
            "95\n",
            "1\n",
            "7 4 Loss=nan Time=12.225\n",
            "90\n",
            "1\n",
            "7 5 Loss=nan Time=12.206\n",
            "117\n",
            "1\n",
            "7 6 Loss=nan Time=11.763\n",
            "122\n",
            "1\n",
            "7 7 Loss=nan Time=10.367\n",
            "81\n",
            "1\n",
            "7 8 Loss=nan Time=12.190\n",
            "85\n",
            "1\n",
            "7 9 Loss=nan Time=12.385\n",
            "124\n",
            "1\n",
            "7 10 Loss=nan Time=12.255\n",
            "114\n",
            "1\n",
            "7 11 Loss=nan Time=10.854\n",
            "75\n",
            "1\n",
            "7 12 Loss=nan Time=11.611\n",
            "127\n",
            "1\n",
            "7 13 Loss=nan Time=12.299\n",
            "129\n",
            "1\n",
            "7 14 Loss=nan Time=12.293\n",
            "73\n",
            "1\n",
            "7 15 Loss=nan Time=12.185\n",
            "102\n",
            "1\n",
            "7 16 Loss=nan Time=10.421\n",
            "91\n",
            "1\n",
            "7 17 Loss=nan Time=11.989\n",
            "92\n",
            "1\n",
            "7 18 Loss=nan Time=12.331\n",
            "113\n",
            "1\n",
            "7 19 Loss=nan Time=12.313\n",
            "112\n",
            "1\n",
            "7 20 Loss=nan Time=11.880\n",
            "88\n",
            "1\n",
            "7 21 Loss=nan Time=10.464\n",
            "130\n",
            "1\n",
            "7 22 Loss=nan Time=12.277\n",
            "86\n",
            "1\n",
            "7 23 Loss=nan Time=12.321\n",
            "100\n",
            "1\n",
            "7 24 Loss=nan Time=12.260\n",
            "94\n",
            "1\n",
            "7 25 Loss=nan Time=11.290\n",
            "109\n",
            "1\n",
            "7 26 Loss=nan Time=10.985\n",
            "121\n",
            "1\n",
            "7 27 Loss=nan Time=12.100\n",
            "104\n",
            "1\n",
            "7 28 Loss=nan Time=12.272\n",
            "119\n",
            "1\n",
            "7 29 Loss=nan Time=12.240\n",
            "108\n",
            "1\n",
            "7 30 Loss=nan Time=10.369\n",
            "84\n",
            "1\n",
            "7 31 Loss=nan Time=11.934\n",
            "72\n",
            "1\n",
            "7 32 Loss=nan Time=12.229\n",
            "128\n",
            "1\n",
            "7 33 Loss=nan Time=12.296\n",
            "83\n",
            "1\n",
            "7 34 Loss=nan Time=11.979\n",
            "96\n",
            "1\n",
            "7 35 Loss=nan Time=10.274\n",
            "118\n",
            "1\n",
            "7 36 Loss=nan Time=12.246\n",
            "98\n",
            "1\n",
            "7 37 Loss=nan Time=12.223\n",
            "76\n",
            "1\n",
            "7 38 Loss=nan Time=12.233\n",
            "123\n",
            "1\n",
            "7 39 Loss=nan Time=10.840\n",
            "99\n",
            "1\n",
            "7 40 Loss=nan Time=11.404\n",
            "122\n",
            "1\n",
            "8 1 Loss=nan Time=12.589\n",
            "108\n",
            "1\n",
            "8 2 Loss=nan Time=12.440\n",
            "119\n",
            "1\n",
            "8 3 Loss=nan Time=12.319\n",
            "99\n",
            "1\n",
            "8 4 Loss=nan Time=10.950\n",
            "117\n",
            "1\n",
            "8 5 Loss=nan Time=11.407\n",
            "118\n",
            "1\n",
            "8 6 Loss=nan Time=12.229\n",
            "113\n",
            "1\n",
            "8 7 Loss=nan Time=12.273\n",
            "73\n",
            "1\n",
            "8 8 Loss=nan Time=12.182\n",
            "121\n",
            "1\n",
            "8 9 Loss=nan Time=10.124\n",
            "110\n",
            "1\n",
            "8 10 Loss=nan Time=12.079\n",
            "124\n",
            "1\n",
            "8 11 Loss=nan Time=12.319\n",
            "83\n",
            "1\n",
            "8 12 Loss=nan Time=12.288\n",
            "81\n",
            "1\n",
            "8 13 Loss=nan Time=12.489\n",
            "109\n",
            "1\n",
            "8 14 Loss=nan Time=10.402\n",
            "95\n",
            "1\n",
            "8 15 Loss=nan Time=12.216\n",
            "78\n",
            "1\n",
            "8 16 Loss=nan Time=12.178\n",
            "130\n",
            "1\n",
            "8 17 Loss=nan Time=12.268\n",
            "129\n",
            "1\n",
            "8 18 Loss=nan Time=11.182\n",
            "100\n",
            "1\n",
            "8 19 Loss=nan Time=11.033\n",
            "127\n",
            "1\n",
            "8 20 Loss=nan Time=12.148\n",
            "112\n",
            "1\n",
            "8 21 Loss=nan Time=12.251\n",
            "102\n",
            "1\n",
            "8 22 Loss=nan Time=12.189\n",
            "94\n",
            "1\n",
            "8 23 Loss=nan Time=10.066\n",
            "72\n",
            "1\n",
            "8 24 Loss=nan Time=12.255\n",
            "84\n",
            "1\n",
            "8 25 Loss=nan Time=12.174\n",
            "97\n",
            "1\n",
            "8 26 Loss=nan Time=12.100\n",
            "90\n",
            "1\n",
            "8 27 Loss=nan Time=11.277\n",
            "86\n",
            "1\n",
            "8 28 Loss=nan Time=10.878\n",
            "98\n",
            "1\n",
            "8 29 Loss=nan Time=12.381\n",
            "85\n",
            "1\n",
            "8 30 Loss=nan Time=12.212\n",
            "88\n",
            "1\n",
            "8 31 Loss=nan Time=12.225\n",
            "96\n",
            "1\n",
            "8 32 Loss=nan Time=10.408\n",
            "76\n",
            "1\n",
            "8 33 Loss=nan Time=11.990\n",
            "75\n",
            "1\n",
            "8 34 Loss=nan Time=12.257\n",
            "92\n",
            "1\n",
            "8 35 Loss=nan Time=12.382\n",
            "104\n",
            "1\n",
            "8 36 Loss=nan Time=11.626\n",
            "123\n",
            "1\n",
            "8 37 Loss=nan Time=10.467\n",
            "114\n",
            "1\n",
            "8 38 Loss=nan Time=12.249\n",
            "128\n",
            "1\n",
            "8 39 Loss=nan Time=12.398\n",
            "91\n",
            "1\n",
            "8 40 Loss=nan Time=12.367\n",
            "117\n",
            "1\n",
            "9 1 Loss=nan Time=11.999\n",
            "122\n",
            "1\n",
            "9 2 Loss=nan Time=10.731\n",
            "92\n",
            "1\n",
            "9 3 Loss=nan Time=12.457\n",
            "100\n",
            "1\n",
            "9 4 Loss=nan Time=12.395\n",
            "81\n",
            "1\n",
            "9 5 Loss=nan Time=12.300\n",
            "102\n",
            "1\n",
            "9 6 Loss=nan Time=11.418\n",
            "124\n",
            "1\n",
            "9 7 Loss=nan Time=10.896\n",
            "130\n",
            "1\n",
            "9 8 Loss=nan Time=12.312\n",
            "88\n",
            "1\n",
            "9 9 Loss=nan Time=12.324\n",
            "123\n",
            "1\n",
            "9 10 Loss=nan Time=12.229\n",
            "113\n",
            "1\n",
            "9 11 Loss=nan Time=10.746\n",
            "118\n",
            "1\n",
            "9 12 Loss=nan Time=11.475\n",
            "96\n",
            "1\n",
            "9 13 Loss=nan Time=12.262\n",
            "127\n",
            "1\n",
            "9 14 Loss=nan Time=12.348\n",
            "85\n",
            "1\n",
            "9 15 Loss=nan Time=12.344\n",
            "119\n",
            "1\n",
            "9 16 Loss=nan Time=10.298\n",
            "129\n",
            "1\n",
            "9 17 Loss=nan Time=11.963\n",
            "114\n",
            "1\n",
            "9 18 Loss=nan Time=12.350\n",
            "84\n",
            "1\n",
            "9 19 Loss=nan Time=12.248\n",
            "78\n",
            "1\n",
            "9 20 Loss=nan Time=12.234\n",
            "110\n",
            "1\n",
            "9 21 Loss=nan Time=10.304\n",
            "112\n",
            "1\n",
            "9 22 Loss=nan Time=12.260\n",
            "109\n",
            "1\n",
            "9 23 Loss=nan Time=12.147\n",
            "91\n",
            "1\n",
            "9 24 Loss=nan Time=12.199\n",
            "108\n",
            "1\n",
            "9 25 Loss=nan Time=11.929\n",
            "97\n",
            "1\n",
            "9 26 Loss=nan Time=10.712\n",
            "90\n",
            "1\n",
            "9 27 Loss=nan Time=12.245\n",
            "86\n",
            "1\n",
            "9 28 Loss=nan Time=12.249\n",
            "83\n",
            "1\n",
            "9 29 Loss=nan Time=12.174\n",
            "104\n",
            "1\n",
            "9 30 Loss=nan Time=11.231\n",
            "94\n",
            "1\n",
            "9 31 Loss=nan Time=10.914\n",
            "73\n",
            "1\n",
            "9 32 Loss=nan Time=12.225\n",
            "128\n",
            "1\n",
            "9 33 Loss=nan Time=12.302\n",
            "95\n",
            "1\n",
            "9 34 Loss=nan Time=12.225\n",
            "76\n",
            "1\n",
            "9 35 Loss=nan Time=11.510\n",
            "99\n",
            "1\n",
            "9 36 Loss=nan Time=11.424\n",
            "98\n",
            "1\n",
            "9 37 Loss=nan Time=12.162\n",
            "121\n",
            "1\n",
            "9 38 Loss=nan Time=12.200\n",
            "72\n",
            "1\n",
            "9 39 Loss=nan Time=12.271\n",
            "75\n",
            "1\n",
            "9 40 Loss=nan Time=10.278\n",
            "117\n",
            "1\n",
            "10 1 Loss=nan Time=12.158\n",
            "85\n",
            "1\n",
            "10 2 Loss=nan Time=12.338\n",
            "110\n",
            "1\n",
            "10 3 Loss=nan Time=12.368\n",
            "99\n",
            "1\n",
            "10 4 Loss=nan Time=12.319\n",
            "86\n",
            "1\n",
            "10 5 Loss=nan Time=10.641\n",
            "88\n",
            "1\n",
            "10 6 Loss=nan Time=11.728\n",
            "84\n",
            "1\n",
            "10 7 Loss=nan Time=12.220\n",
            "124\n",
            "1\n",
            "10 8 Loss=nan Time=12.539\n",
            "98\n",
            "1\n",
            "10 9 Loss=nan Time=12.315\n",
            "118\n",
            "1\n",
            "10 10 Loss=nan Time=10.607\n",
            "96\n",
            "1\n",
            "10 11 Loss=nan Time=12.055\n",
            "73\n",
            "1\n",
            "10 12 Loss=nan Time=12.605\n",
            "104\n",
            "1\n",
            "10 13 Loss=nan Time=12.516\n",
            "100\n",
            "1\n",
            "10 14 Loss=nan Time=12.474\n",
            "81\n",
            "1\n",
            "10 15 Loss=nan Time=10.919\n",
            "109\n",
            "1\n",
            "10 16 Loss=nan Time=11.475\n",
            "72\n",
            "1\n",
            "10 17 Loss=nan Time=12.314\n",
            "129\n",
            "1\n",
            "10 18 Loss=nan Time=12.399\n",
            "119\n",
            "1\n",
            "10 19 Loss=nan Time=12.312\n",
            "128\n",
            "1\n",
            "10 20 Loss=nan Time=10.488\n",
            "130\n",
            "1\n",
            "10 21 Loss=nan Time=11.929\n",
            "123\n",
            "1\n",
            "10 22 Loss=nan Time=12.389\n",
            "121\n",
            "1\n",
            "10 23 Loss=nan Time=12.360\n",
            "75\n",
            "1\n",
            "10 24 Loss=nan Time=12.334\n",
            "113\n",
            "1\n",
            "10 25 Loss=nan Time=10.614\n",
            "94\n",
            "1\n",
            "10 26 Loss=nan Time=11.680\n",
            "78\n",
            "1\n",
            "10 27 Loss=nan Time=12.198\n",
            "114\n",
            "1\n",
            "10 28 Loss=nan Time=12.294\n",
            "122\n",
            "1\n",
            "10 29 Loss=nan Time=12.345\n",
            "97\n",
            "1\n",
            "10 30 Loss=nan Time=10.515\n",
            "90\n",
            "1\n",
            "10 31 Loss=nan Time=11.892\n",
            "112\n",
            "1\n",
            "10 32 Loss=nan Time=12.551\n",
            "102\n",
            "1\n",
            "10 33 Loss=nan Time=12.183\n",
            "108\n",
            "1\n",
            "10 34 Loss=nan Time=12.324\n",
            "95\n",
            "1\n",
            "10 35 Loss=nan Time=10.743\n",
            "127\n",
            "1\n",
            "10 36 Loss=nan Time=11.724\n",
            "92\n",
            "1\n",
            "10 37 Loss=nan Time=12.228\n",
            "91\n",
            "1\n",
            "10 38 Loss=nan Time=12.254\n",
            "76\n",
            "1\n",
            "10 39 Loss=nan Time=12.367\n",
            "83\n",
            "1\n",
            "10 40 Loss=nan Time=10.368\n",
            "72\n",
            "1\n",
            "11 1 Loss=nan Time=11.895\n",
            "95\n",
            "1\n",
            "11 2 Loss=nan Time=12.266\n",
            "127\n",
            "1\n",
            "11 3 Loss=nan Time=12.322\n",
            "113\n",
            "1\n",
            "11 4 Loss=nan Time=12.317\n",
            "109\n",
            "1\n",
            "11 5 Loss=nan Time=10.377\n",
            "85\n",
            "1\n",
            "11 6 Loss=nan Time=11.937\n",
            "118\n",
            "1\n",
            "11 7 Loss=nan Time=12.262\n",
            "97\n",
            "1\n",
            "11 8 Loss=nan Time=12.360\n",
            "76\n",
            "1\n",
            "11 9 Loss=nan Time=12.186\n",
            "91\n",
            "1\n",
            "11 10 Loss=nan Time=10.229\n",
            "92\n",
            "1\n",
            "11 11 Loss=nan Time=12.463\n",
            "99\n",
            "1\n",
            "11 12 Loss=nan Time=12.273\n",
            "88\n",
            "1\n",
            "11 13 Loss=nan Time=12.244\n",
            "124\n",
            "1\n",
            "11 14 Loss=nan Time=11.469\n",
            "78\n",
            "1\n",
            "11 15 Loss=nan Time=10.917\n",
            "73\n",
            "1\n",
            "11 16 Loss=nan Time=12.787\n",
            "83\n",
            "1\n",
            "11 17 Loss=nan Time=12.185\n",
            "110\n",
            "1\n",
            "11 18 Loss=nan Time=12.209\n",
            "100\n",
            "1\n",
            "11 19 Loss=nan Time=11.027\n",
            "128\n",
            "1\n",
            "11 20 Loss=nan Time=11.412\n",
            "90\n",
            "1\n",
            "11 21 Loss=nan Time=12.297\n",
            "112\n",
            "1\n",
            "11 22 Loss=nan Time=12.311\n",
            "108\n",
            "1\n",
            "11 23 Loss=nan Time=12.315\n",
            "98\n",
            "1\n",
            "11 24 Loss=nan Time=10.446\n",
            "86\n",
            "1\n",
            "11 25 Loss=nan Time=12.050\n",
            "130\n",
            "1\n",
            "11 26 Loss=nan Time=12.413\n",
            "114\n",
            "1\n",
            "11 27 Loss=nan Time=12.342\n",
            "75\n",
            "1\n",
            "11 28 Loss=nan Time=12.159\n",
            "104\n",
            "1\n",
            "11 29 Loss=nan Time=10.091\n",
            "121\n",
            "1\n",
            "11 30 Loss=nan Time=12.405\n",
            "129\n",
            "1\n",
            "11 31 Loss=nan Time=12.307\n",
            "102\n",
            "1\n",
            "11 32 Loss=nan Time=12.350\n",
            "96\n",
            "1\n",
            "11 33 Loss=nan Time=11.996\n",
            "84\n",
            "1\n",
            "11 34 Loss=nan Time=10.543\n",
            "123\n",
            "1\n",
            "11 35 Loss=nan Time=12.458\n",
            "117\n",
            "1\n",
            "11 36 Loss=nan Time=12.436\n",
            "119\n",
            "1\n",
            "11 37 Loss=nan Time=12.341\n",
            "94\n",
            "1\n",
            "11 38 Loss=nan Time=11.326\n",
            "81\n",
            "1\n",
            "11 39 Loss=nan Time=11.079\n",
            "122\n",
            "1\n",
            "11 40 Loss=nan Time=12.380\n",
            "85\n",
            "1\n",
            "12 1 Loss=nan Time=12.633\n",
            "109\n",
            "1\n",
            "12 2 Loss=nan Time=12.343\n",
            "91\n",
            "1\n",
            "12 3 Loss=nan Time=12.028\n",
            "100\n",
            "1\n",
            "12 4 Loss=nan Time=10.608\n",
            "119\n",
            "1\n",
            "12 5 Loss=nan Time=12.386\n",
            "75\n",
            "1\n",
            "12 6 Loss=nan Time=12.411\n",
            "121\n",
            "1\n",
            "12 7 Loss=nan Time=12.378\n",
            "99\n",
            "1\n",
            "12 8 Loss=nan Time=11.612\n",
            "96\n",
            "1\n",
            "12 9 Loss=nan Time=10.862\n",
            "78\n",
            "1\n",
            "12 10 Loss=nan Time=12.376\n",
            "123\n",
            "1\n",
            "12 11 Loss=nan Time=12.328\n",
            "98\n",
            "1\n",
            "12 12 Loss=nan Time=12.295\n",
            "122\n",
            "1\n",
            "12 13 Loss=nan Time=11.249\n",
            "73\n",
            "1\n",
            "12 14 Loss=nan Time=11.113\n",
            "113\n",
            "1\n",
            "12 15 Loss=nan Time=12.204\n",
            "97\n",
            "1\n",
            "12 16 Loss=nan Time=12.405\n",
            "76\n",
            "1\n",
            "12 17 Loss=nan Time=12.245\n",
            "94\n",
            "1\n",
            "12 18 Loss=nan Time=10.653\n",
            "81\n",
            "1\n",
            "12 19 Loss=nan Time=11.649\n",
            "114\n",
            "1\n",
            "12 20 Loss=nan Time=12.313\n",
            "104\n",
            "1\n",
            "12 21 Loss=nan Time=12.338\n",
            "124\n",
            "1\n",
            "12 22 Loss=nan Time=12.250\n",
            "92\n",
            "1\n",
            "12 23 Loss=nan Time=10.491\n",
            "86\n",
            "1\n",
            "12 24 Loss=nan Time=11.968\n",
            "95\n",
            "1\n",
            "12 25 Loss=nan Time=12.286\n",
            "130\n",
            "1\n",
            "12 26 Loss=nan Time=12.323\n",
            "110\n",
            "1\n",
            "12 27 Loss=nan Time=12.209\n",
            "72\n",
            "1\n",
            "12 28 Loss=nan Time=10.314\n",
            "127\n",
            "1\n",
            "12 29 Loss=nan Time=12.220\n",
            "118\n",
            "1\n",
            "12 30 Loss=nan Time=12.459\n",
            "128\n",
            "1\n",
            "12 31 Loss=nan Time=12.162\n",
            "88\n",
            "1\n",
            "12 32 Loss=nan Time=11.117\n",
            "112\n",
            "1\n",
            "12 33 Loss=nan Time=11.182\n",
            "108\n",
            "1\n",
            "12 34 Loss=nan Time=12.338\n",
            "90\n",
            "1\n",
            "12 35 Loss=nan Time=12.379\n",
            "117\n",
            "1\n",
            "12 36 Loss=nan Time=12.168\n",
            "102\n",
            "1\n",
            "12 37 Loss=nan Time=10.319\n",
            "83\n",
            "1\n",
            "12 38 Loss=nan Time=12.790\n",
            "129\n",
            "1\n",
            "12 39 Loss=nan Time=12.253\n",
            "84\n",
            "1\n",
            "12 40 Loss=nan Time=12.401\n",
            "76\n",
            "1\n",
            "13 1 Loss=nan Time=12.722\n",
            "83\n",
            "1\n",
            "13 2 Loss=nan Time=10.319\n",
            "118\n",
            "1\n",
            "13 3 Loss=nan Time=11.952\n",
            "95\n",
            "1\n",
            "13 4 Loss=nan Time=12.389\n",
            "112\n",
            "1\n",
            "13 5 Loss=nan Time=12.348\n",
            "119\n",
            "1\n",
            "13 6 Loss=nan Time=11.884\n",
            "123\n",
            "1\n",
            "13 7 Loss=nan Time=10.091\n",
            "81\n",
            "1\n",
            "13 8 Loss=nan Time=12.202\n",
            "122\n",
            "1\n",
            "13 9 Loss=nan Time=12.276\n",
            "97\n",
            "1\n",
            "13 10 Loss=nan Time=12.198\n",
            "98\n",
            "1\n",
            "13 11 Loss=nan Time=11.092\n",
            "127\n",
            "1\n",
            "13 12 Loss=nan Time=11.060\n",
            "72\n",
            "1\n",
            "13 13 Loss=nan Time=12.096\n",
            "113\n",
            "1\n",
            "13 14 Loss=nan Time=12.125\n",
            "86\n",
            "1\n",
            "13 15 Loss=nan Time=12.058\n",
            "96\n",
            "1\n",
            "13 16 Loss=nan Time=9.906\n",
            "114\n",
            "1\n",
            "13 17 Loss=nan Time=11.951\n",
            "104\n",
            "1\n",
            "13 18 Loss=nan Time=12.141\n",
            "109\n",
            "1\n",
            "13 19 Loss=nan Time=11.994\n",
            "102\n",
            "1\n",
            "13 20 Loss=nan Time=10.580\n",
            "130\n",
            "1\n",
            "13 21 Loss=nan Time=11.462\n",
            "91\n",
            "1\n",
            "13 22 Loss=nan Time=12.092\n",
            "75\n",
            "1\n",
            "13 23 Loss=nan Time=12.092\n",
            "73\n",
            "1\n",
            "13 24 Loss=nan Time=11.771\n",
            "100\n",
            "1\n",
            "13 25 Loss=nan Time=10.179\n",
            "99\n",
            "1\n",
            "13 26 Loss=nan Time=12.212\n",
            "92\n",
            "1\n",
            "13 27 Loss=nan Time=12.210\n",
            "78\n",
            "1\n",
            "13 28 Loss=nan Time=12.165\n",
            "85\n",
            "1\n",
            "13 29 Loss=nan Time=10.924\n",
            "121\n",
            "1\n",
            "13 30 Loss=nan Time=11.396\n",
            "94\n",
            "1\n",
            "13 31 Loss=nan Time=12.132\n",
            "117\n",
            "1\n",
            "13 32 Loss=nan Time=12.105\n",
            "124\n",
            "1\n",
            "13 33 Loss=nan Time=12.150\n",
            "84\n",
            "1\n",
            "13 34 Loss=nan Time=10.056\n",
            "128\n",
            "1\n",
            "13 35 Loss=nan Time=12.054\n",
            "90\n",
            "1\n",
            "13 36 Loss=nan Time=12.218\n",
            "129\n",
            "1\n",
            "13 37 Loss=nan Time=12.083\n",
            "88\n",
            "1\n",
            "13 38 Loss=nan Time=11.091\n",
            "110\n",
            "1\n",
            "13 39 Loss=nan Time=11.073\n",
            "108\n",
            "1\n",
            "13 40 Loss=nan Time=12.084\n",
            "128\n",
            "1\n",
            "14 1 Loss=nan Time=12.598\n",
            "108\n",
            "1\n",
            "14 2 Loss=nan Time=12.153\n",
            "129\n",
            "1\n",
            "14 3 Loss=nan Time=11.064\n",
            "114\n",
            "1\n",
            "14 4 Loss=nan Time=11.352\n",
            "113\n",
            "1\n",
            "14 5 Loss=nan Time=12.124\n",
            "102\n",
            "1\n",
            "14 6 Loss=nan Time=12.207\n",
            "94\n",
            "1\n",
            "14 7 Loss=nan Time=12.234\n",
            "83\n",
            "1\n",
            "14 8 Loss=nan Time=10.256\n",
            "73\n",
            "1\n",
            "14 9 Loss=nan Time=11.756\n",
            "72\n",
            "1\n",
            "14 10 Loss=nan Time=12.103\n",
            "76\n",
            "1\n",
            "14 11 Loss=nan Time=12.062\n",
            "85\n",
            "1\n",
            "14 12 Loss=nan Time=11.556\n",
            "95\n",
            "1\n",
            "14 13 Loss=nan Time=10.557\n",
            "91\n",
            "1\n",
            "14 14 Loss=nan Time=12.226\n",
            "104\n",
            "1\n",
            "14 15 Loss=nan Time=12.278\n",
            "130\n",
            "1\n",
            "14 16 Loss=nan Time=12.205\n",
            "112\n",
            "1\n",
            "14 17 Loss=nan Time=10.567\n",
            "75\n",
            "1\n",
            "14 18 Loss=nan Time=11.551\n",
            "78\n",
            "1\n",
            "14 19 Loss=nan Time=12.194\n",
            "88\n",
            "1\n",
            "14 20 Loss=nan Time=12.238\n",
            "84\n",
            "1\n",
            "14 21 Loss=nan Time=11.804\n",
            "96\n",
            "1\n",
            "14 22 Loss=nan Time=10.430\n",
            "127\n",
            "1\n",
            "14 23 Loss=nan Time=12.212\n",
            "100\n",
            "1\n",
            "14 24 Loss=nan Time=12.268\n",
            "119\n",
            "1\n",
            "14 25 Loss=nan Time=12.242\n",
            "98\n",
            "1\n",
            "14 26 Loss=nan Time=11.397\n",
            "124\n",
            "1\n",
            "14 27 Loss=nan Time=10.999\n",
            "121\n",
            "1\n",
            "14 28 Loss=nan Time=12.149\n",
            "86\n",
            "1\n",
            "14 29 Loss=nan Time=12.148\n",
            "122\n",
            "1\n",
            "14 30 Loss=nan Time=12.115\n",
            "117\n",
            "1\n",
            "14 31 Loss=nan Time=10.312\n",
            "92\n",
            "1\n",
            "14 32 Loss=nan Time=11.664\n",
            "97\n",
            "1\n",
            "14 33 Loss=nan Time=12.179\n",
            "118\n",
            "1\n",
            "14 34 Loss=nan Time=12.045\n",
            "123\n",
            "1\n",
            "14 35 Loss=nan Time=11.372\n",
            "90\n",
            "1\n",
            "14 36 Loss=nan Time=10.663\n",
            "99\n",
            "1\n",
            "14 37 Loss=nan Time=12.074\n",
            "109\n",
            "1\n",
            "14 38 Loss=nan Time=12.133\n",
            "81\n",
            "1\n",
            "14 39 Loss=nan Time=12.018\n",
            "110\n",
            "1\n",
            "14 40 Loss=nan Time=9.989\n",
            "88\n",
            "1\n",
            "15 1 Loss=nan Time=11.798\n",
            "109\n",
            "1\n",
            "15 2 Loss=nan Time=13.210\n",
            "114\n",
            "1\n",
            "15 3 Loss=nan Time=12.243\n",
            "102\n",
            "1\n",
            "15 4 Loss=nan Time=12.194\n",
            "118\n",
            "1\n",
            "15 5 Loss=nan Time=10.524\n",
            "100\n",
            "1\n",
            "15 6 Loss=nan Time=11.654\n",
            "113\n",
            "1\n",
            "15 7 Loss=nan Time=12.237\n",
            "83\n",
            "1\n",
            "15 8 Loss=nan Time=12.146\n",
            "81\n",
            "1\n",
            "15 9 Loss=nan Time=12.072\n",
            "119\n",
            "1\n",
            "15 10 Loss=nan Time=10.209\n",
            "96\n",
            "1\n",
            "15 11 Loss=nan Time=11.783\n",
            "92\n",
            "1\n",
            "15 12 Loss=nan Time=12.238\n",
            "124\n",
            "1\n",
            "15 13 Loss=nan Time=12.206\n",
            "73\n",
            "1\n",
            "15 14 Loss=nan Time=11.466\n",
            "123\n",
            "1\n",
            "15 15 Loss=nan Time=10.561\n",
            "98\n",
            "1\n",
            "15 16 Loss=nan Time=12.161\n",
            "91\n",
            "1\n",
            "15 17 Loss=nan Time=12.095\n",
            "130\n",
            "1\n",
            "15 18 Loss=nan Time=12.223\n",
            "128\n",
            "1\n",
            "15 19 Loss=nan Time=10.937\n",
            "72\n",
            "1\n",
            "15 20 Loss=nan Time=10.875\n",
            "97\n",
            "1\n",
            "15 21 Loss=nan Time=12.141\n",
            "94\n",
            "1\n",
            "15 22 Loss=nan Time=12.173\n",
            "127\n",
            "1\n",
            "15 23 Loss=nan Time=12.140\n",
            "90\n",
            "1\n",
            "15 24 Loss=nan Time=10.326\n",
            "76\n",
            "1\n",
            "15 25 Loss=nan Time=11.677\n",
            "104\n",
            "1\n",
            "15 26 Loss=nan Time=12.292\n",
            "85\n",
            "1\n",
            "15 27 Loss=nan Time=12.185\n",
            "95\n",
            "1\n",
            "15 28 Loss=nan Time=12.009\n",
            "99\n",
            "1\n",
            "15 29 Loss=nan Time=10.174\n",
            "112\n",
            "1\n",
            "15 30 Loss=nan Time=12.150\n",
            "122\n",
            "1\n",
            "15 31 Loss=nan Time=12.115\n",
            "75\n",
            "1\n",
            "15 32 Loss=nan Time=12.146\n",
            "117\n",
            "1\n",
            "15 33 Loss=nan Time=11.254\n",
            "129\n",
            "1\n",
            "15 34 Loss=nan Time=10.759\n",
            "78\n",
            "1\n",
            "15 35 Loss=nan Time=12.117\n",
            "84\n",
            "1\n",
            "15 36 Loss=nan Time=12.201\n",
            "110\n",
            "1\n",
            "15 37 Loss=nan Time=12.143\n",
            "86\n",
            "1\n",
            "15 38 Loss=nan Time=10.489\n",
            "121\n",
            "1\n",
            "15 39 Loss=nan Time=11.581\n",
            "108\n",
            "1\n",
            "15 40 Loss=nan Time=12.198\n"
          ]
        }
      ],
      "source": [
        "G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n",
        "\n",
        "t_vars = tf.trainable_variables()\n",
        "lr = tf.placeholder(tf.float32)\n",
        "# G_opt = tf.train.AdagradOptimizer(learning_rate=lr).minimize(G_loss)\n",
        "# G_opt = tf.compat.v1.train.AdadeltaOptimizer(learning_rate=1.0).minimize(G_loss)\n",
        "G_opt = tf.compat.v1.train.RMSPropOptimizer(learning_rate=0.1).minimize(G_loss) \n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "if ckpt:\n",
        "    print('loaded ' + ckpt.model_checkpoint_path)\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "# Raw data takes long time to load. Keep them in memory after loaded.\n",
        "gt_images = [None] * 6000\n",
        "input_images = {}\n",
        "input_images['300'] = [None] * len(train_ids)\n",
        "input_images['250'] = [None] * len(train_ids)\n",
        "input_images['100'] = [None] * len(train_ids)\n",
        "\n",
        "g_loss = np.zeros((5000, 1))\n",
        "\n",
        "allfolders = glob.glob(result_dir + '*0')\n",
        "lastepoch = 0\n",
        "for folder in allfolders:\n",
        "    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n",
        "\n",
        "losses = []\n",
        "times = []\n",
        "\n",
        "print('Debug scheme: ', DEBUG, 'Nr. images: ', len(train_ids))\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for epoch in range(lastepoch, num_epochs+1):\n",
        "    if os.path.isdir(result_dir + '%04d' % epoch):\n",
        "        continue\n",
        "    cnt = 0\n",
        "    if epoch > 2000:\n",
        "        learning_rate = 1e-5\n",
        "\n",
        "    for ind in np.random.permutation(len(train_ids)):\n",
        "        # get the path from image id\n",
        "        train_id = train_ids[ind]\n",
        "        print(train_id)\n",
        "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
        "        print(len(in_files))\n",
        "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
        "        in_fn = os.path.basename(in_path)\n",
        "\n",
        "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
        "        gt_path = gt_files[0]\n",
        "        gt_fn = os.path.basename(gt_path)\n",
        "        in_exposure = float(in_fn[9:-5])\n",
        "        gt_exposure = float(gt_fn[9:-5])\n",
        "        ratio = min(gt_exposure / in_exposure, 300)\n",
        "\n",
        "        st = time.time()\n",
        "        cnt += 1\n",
        "\n",
        "        if input_images[str(ratio)[0:3]][ind] is None:\n",
        "            raw = rawpy.imread(in_path)\n",
        "            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
        "\n",
        "            gt_raw = rawpy.imread(gt_path)\n",
        "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
        "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
        "\n",
        "        # crop\n",
        "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
        "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
        "\n",
        "        xx = np.random.randint(0, W - ps)\n",
        "        yy = np.random.randint(0, H - ps)\n",
        "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
        "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
        "\n",
        "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
        "            input_patch = np.flip(input_patch, axis=1)\n",
        "            gt_patch = np.flip(gt_patch, axis=1)\n",
        "        if np.random.randint(2, size=1)[0] == 1:\n",
        "            input_patch = np.flip(input_patch, axis=2)\n",
        "            gt_patch = np.flip(gt_patch, axis=2)\n",
        "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
        "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
        "\n",
        "        input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n",
        "                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n",
        "        output = np.minimum(np.maximum(output, 0), 1)\n",
        "        g_loss[ind] = G_current\n",
        "\n",
        "        print(\"%d %d Loss=%.3f Time=%.3f\" % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n",
        "        losses.append(np.mean(g_loss[np.where(g_loss)]))\n",
        "        times.append(time.time() - st)\n",
        "        if epoch % save_freq == 0:\n",
        "            if not os.path.isdir(result_dir + '%04d' % epoch):\n",
        "                os.makedirs(result_dir + '%04d' % epoch)\n",
        "\n",
        "            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n",
        "            toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n",
        "                result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n",
        "\n",
        "    saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mlOvlyO3F378",
        "outputId": "b8618665-44e1-4007-f0e8-8fe7b6bf16cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./checkpoint/RMSprop_lr0.1/model.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "saver.save(sess, checkpoint_dir + 'model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zY0ysN8J6x2"
      },
      "outputs": [],
      "source": [
        "np.save(checkpoint_dir + \"losses\", losses)\n",
        "np.save(checkpoint_dir + \"times\", times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp1YHm6TKRf0",
        "outputId": "93eaf7b2-2238-402a-b0ca-59d2680f1e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(640,)\n"
          ]
        }
      ],
      "source": [
        "train_losses = np.load(checkpoint_dir + \"losses.npy\")\n",
        "print(train_losses.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "G2L9yaNeqd3c",
        "outputId": "0fa862da-cff1-4d84-b42f-43c7b5c3b68c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/aElEQVR4nO3deXhU9d3+8Xsmy2RfSGBCQiBsEvYgAQyouERxqYpLiy0VjBbrrr88tpWq4PLQ0GottfKAWtHWDapV6opKFBRFwr7vW8KSDci+z5zfH4HBlIAQZ3LCyft1XXNJznxn5jNzkLnz3Y7NMAxDAAAAFmE3uwAAAABvItwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLaRPhZubMmUpKSlJQUJBGjBihnJyck7Z99dVXZbPZmtyCgoJasVoAANCWmR5u5s2bp8zMTE2dOlWrVq3S4MGDNWbMGBUWFp70MRERETp48KDntnfv3lasGAAAtGWmh5tnn31WkyZNUkZGhvr166fZs2crJCREc+bMOeljbDab4uLiPDen09mKFQMAgLbM38wXr6ur08qVKzV58mTPMbvdrvT0dC1duvSkj6uoqFC3bt3kdrt17rnn6g9/+IP69+/fbNva2lrV1tZ6fna73Tp8+LBiYmJks9m892YAAIDPGIah8vJyxcfHy24/dd+MqeGmuLhYLpfrhJ4Xp9OpLVu2NPuYPn36aM6cORo0aJBKS0v1zDPPaOTIkdq4caO6dOlyQvusrCw98cQTPqkfAAC0rry8vGa/77/P1HDTEmlpaUpLS/P8PHLkSPXt21cvvPCCnnrqqRPaT548WZmZmZ6fS0tL1bVrV+Xl5SkiIqJVagYAoC2a8fk2/X3Jbo0/r6smX9nX7HJOqaysTImJiQoPD//BtqaGm9jYWPn5+amgoKDJ8YKCAsXFxZ3WcwQEBGjIkCHasWNHs/c7HA45HI4TjkdERBBuAADtWlBomOyOEAWFhJ0134mnM6XE1AnFgYGBGjp0qLKzsz3H3G63srOzm/TOnIrL5dL69evVuXNnX5UJAADOIqYPS2VmZmrixIlKTU3V8OHDNWPGDFVWViojI0OSNGHCBCUkJCgrK0uS9OSTT+q8885Tr169VFJSoqefflp79+7Vr371KzPfBgAAaCNMDzfjxo1TUVGRpkyZovz8fKWkpGjBggWeSca5ublNZkUfOXJEkyZNUn5+vqKjozV06FB9++236tevn1lvAQAAtCE2wzAMs4toTWVlZYqMjFRpaelZM74IAIAv/GnBFv3fop3KGJWkqdc0v6VKW3Em39+mb+IHAADgTYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAADaqWOXKLDph6+0fTYh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAA0M7ZbGZX4F2EGwAA2inDMLsC3yDcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAADQztnMLsDLCDcAALRThgyzS/AJwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AAO2czWZ2Bd5FuAEAAJZCuAEAAJZCuAEAoL0yzC7ANwg3AADAUgg3AADAUgg3AADAUgg3AADAUtpEuJk5c6aSkpIUFBSkESNGKCcn57QeN3fuXNlsNo0dO9a3BQIAgLOG6eFm3rx5yszM1NSpU7Vq1SoNHjxYY8aMUWFh4Skft2fPHj300EO64IILWqlSAABwNjA93Dz77LOaNGmSMjIy1K9fP82ePVshISGaM2fOSR/jcrk0fvx4PfHEE+rRo0crVgsAANo6U8NNXV2dVq5cqfT0dM8xu92u9PR0LV269KSPe/LJJ9WpUyfdfvvtP/gatbW1Kisra3IDAADWZWq4KS4ulsvlktPpbHLc6XQqPz+/2ccsWbJEL7/8sl566aXTeo2srCxFRkZ6bomJiT+6bgAA0HaZPix1JsrLy3XLLbfopZdeUmxs7Gk9ZvLkySotLfXc8vLyfFwlAAAwk7+ZLx4bGys/Pz8VFBQ0OV5QUKC4uLgT2u/cuVN79uzRNddc4znmdrslSf7+/tq6dat69uzZ5DEOh0MOh8MH1QMAgLbI1J6bwMBADR06VNnZ2Z5jbrdb2dnZSktLO6F9cnKy1q9frzVr1nhu1157rS6++GKtWbOGIScAAFrAZrOZXYJXmdpzI0mZmZmaOHGiUlNTNXz4cM2YMUOVlZXKyMiQJE2YMEEJCQnKyspSUFCQBgwY0OTxUVFRknTCcQAA0D6ZHm7GjRunoqIiTZkyRfn5+UpJSdGCBQs8k4xzc3Nlt59VU4MAAICJTA83knTvvffq3nvvbfa+RYsWnfKxr776qvcLAgAAZy26RAAAaKcMswvwEcINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAADtnM3sAryMcAMAACyFcAMAACyFcAMAACyFcAMAQDtlGIbZJfgE4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAgPbOZnYB3kW4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQCgnTIMsyvwDcINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAADtnE02s0vwKsINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAADtlGF2AT5CuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAoJ2z2cyuwLsINwAAwFIINwAAwFIINwAAwFIINwAAwFLaRLiZOXOmkpKSFBQUpBEjRignJ+ekbd99912lpqYqKipKoaGhSklJ0WuvvdaK1QIAgLbM9HAzb948ZWZmaurUqVq1apUGDx6sMWPGqLCwsNn2HTp00COPPKKlS5dq3bp1ysjIUEZGhj799NNWrhwAALRFpoebZ599VpMmTVJGRob69eun2bNnKyQkRHPmzGm2/UUXXaTrr79effv2Vc+ePfXAAw9o0KBBWrJkSStXDgDA2c0wzK7AN0wNN3V1dVq5cqXS09M9x+x2u9LT07V06dIffLxhGMrOztbWrVt14YUXNtumtrZWZWVlTW4AAMC6TA03xcXFcrlccjqdTY47nU7l5+ef9HGlpaUKCwtTYGCgrr76av3tb3/TZZdd1mzbrKwsRUZGem6JiYlefQ8AAKBtMX1YqiXCw8O1Zs0aLV++XNOmTVNmZqYWLVrUbNvJkyertLTUc8vLy2vdYgEAQKvyN/PFY2Nj5efnp4KCgibHCwoKFBcXd9LH2e129erVS5KUkpKizZs3KysrSxdddNEJbR0OhxwOh1frBgAAbZepPTeBgYEaOnSosrOzPcfcbreys7OVlpZ22s/jdrtVW1vrixIBAMBZxtSeG0nKzMzUxIkTlZqaquHDh2vGjBmqrKxURkaGJGnChAlKSEhQVlaWpMY5NKmpqerZs6dqa2v18ccf67XXXtOsWbPMfBsAAKCNMD3cjBs3TkVFRZoyZYry8/OVkpKiBQsWeCYZ5+bmym4/3sFUWVmpu+++W/v27VNwcLCSk5P1+uuva9y4cWa9BQAA0IbYDMOqq9ybV1ZWpsjISJWWlioiIsLscgAAMM2TH2zSnG926+6Leuq3VySbXc4pncn391m5WgoAAOBkCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAALRThqy5jy/hBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgCAds5mM7sC7yLcAAAASyHcAAAASyHcAAAAS2lRuMnLy9O+ffs8P+fk5OjBBx/Uiy++6LXCAAAAWqJF4eYXv/iFvvzyS0lSfn6+LrvsMuXk5OiRRx7Rk08+6dUCAQAAzkSLws2GDRs0fPhwSdK//vUvDRgwQN9++63eeOMNvfrqq96sDwAA4Iy0KNzU19fL4XBIkhYuXKhrr71WkpScnKyDBw96rzoAAIAz1KJw079/f82ePVtff/21Pv/8c11xxRWSpAMHDigmJsarBQIAAJyJFoWbP/7xj3rhhRd00UUX6ec//7kGDx4sSXr//fc9w1UAAABm8G/Jgy666CIVFxerrKxM0dHRnuN33HGHQkJCvFYcAADwHcMwuwLfaFHPTXV1tWpraz3BZu/evZoxY4a2bt2qTp06ebVAAACAM9GicHPdddfpn//8pySppKREI0aM0J///GeNHTtWs2bN8mqBAAAAZ6JF4WbVqlW64IILJEnvvPOOnE6n9u7dq3/+85967rnnvFogAADAmWhRuKmqqlJ4eLgk6bPPPtMNN9wgu92u8847T3v37vVqgWc7w6oDmgAAtFEtCje9evXS/PnzlZeXp08//VSXX365JKmwsFARERFeLfBsVlPv0uV/+Ur3v7Xa7FIAAGg3WhRupkyZooceekhJSUkaPny40tLSJDX24gwZMsSrBZ7NdhdXanthhT5Yd0DVdS6zywEAoFk22cwuwatatBT8pptu0vnnn6+DBw969riRpEsvvVTXX3+914o721XXNwYaw5B2FlVoQEKkyRUBAGB9LQo3khQXF6e4uDjP1cG7dOnCBn7/5fu9NdsKygk3AAC0ghYNS7ndbj355JOKjIxUt27d1K1bN0VFRempp56S2+32do1nraom4abCxEoAAGg/WtRz88gjj+jll1/W9OnTNWrUKEnSkiVL9Pjjj6umpkbTpk3zapFnq6q6Bs+ftxeUm1gJAADtR4vCzT/+8Q/9/e9/91wNXJIGDRqkhIQE3X333YSbo5oMSxUSbgAAaA0tGpY6fPiwkpOTTzienJysw4cP/+iirOL7w1J5h6tVWdtwitYAAMAbWhRuBg8erOeff/6E488//7wGDRr0o4uyimOrpY7ZUci8GwAAfK1Fw1J/+tOfdPXVV2vhwoWePW6WLl2qvLw8ffzxx14t8Gz2/Tk3UuOKqcGJUeYUAwBAO9GinpvRo0dr27Ztuv7661VSUqKSkhLdcMMN2rhxo1577TVv13jWqq5runJsOz03AAD4XIv3uYmPjz9h4vDatWv18ssv68UXX/zRhVlBdX1jz03nyCAdLK3RNlZMAQDgcy3qucHpOTaheFCXxs37trPXDQAAPke48aHj4SZKkrS/pFoVrJgCAMCnCDc+dGyfm/ioIHUMd0hiMz8AAHztjObc3HDDDae8v6Sk5MfUYjnHVksFB/jrHGeYisprta2gXEO6RptcGQAA1nVG4SYy8tQXfoyMjNSECRN+VEFWcmxYKiTQT707heubHYe4xhQAoM2x2cyuwLvOKNy88sorvqrDko5t4hcS6Kc+ceGSxIopAAB8jDk3PnRszk1woJ/OcYZJYsUUAAC+RrjxoWrPsJS/enVq7LnJL6tRaXW9mWUBAGBphBsfMQxDVd8blooMDlBcRJAkaQdXCAcAwGcINz5S53LL5TYkSUEBfpKk3keHpphUDACA7xBufOTYkJTU2HMjSec4G4emtubTcwMAgK8Qbnzk2DLwAD+bAvwaP+Y+R8PNdoalAADwGcKNjxwLN8FHh6QkhqUAAGgNhBsf+f5KqWN6H+25KSqvVUlVXZP2ZTX12lFYIffReToAAKBlzmgTP5y+72/gd0yYw18JUcHaX1KtbQUVGt69gyRp4aYC3T93tarqXAoP8te5XaOV2i1aQ5OidW7XaM+EZAAA8MMINz7iua5UYNNg0tsZdjTclGtYUrTmfLNH//vRJhmG5Ge3qbymQYu3FWnxtiJJUlJMiN67e5SiQwNb/T0AAHA2YljKR6rrTuy5kY6vmNp8sExT/rNRT33YGGx+PryrNj4xRh/ce76mXtNPVw/qrMjgAO05VKV/Lt3b6vUDAKzPMKw5FYKeGx85NqH4v4eUjoWbN3NyZRiNFyt75Kq+uv387rLZbBrYJVIDu0QqY1R3vb/2gO5/a7Ve/Xa3Jl3Yvcn8HQAA0Dx6bnykqpk5N5I815gyjMaVVC/8cqh+dUEP2Zq5JOtVA+LUtUOIjlTV61/L83xfNAAAFkC48ZHqo3Nu/ru35RxnuDpHBqlzZJDevjNNl/ePO+lz+PvZdceFPSRJL329W/Uut+8KBgC0Wyf+en12I9z4SNX3rgj+fUEBfvryoYu0+DcXa0BC5A8+z01Duyg2zKH9JdX6YO0Bn9QKAICVEG58xDOhuJll3EEBfgr0P72PPijAT7ednyRJmr14J/vgAADwA9pEuJk5c6aSkpIUFBSkESNGKCcn56RtX3rpJV1wwQWKjo5WdHS00tPTT9neLM3tc9NSvzyvm8Id/tpWUKEvthT+6OcDAMDKTA838+bNU2ZmpqZOnapVq1Zp8ODBGjNmjAoLm/8SX7RokX7+85/ryy+/1NKlS5WYmKjLL79c+/fvb+XKT+34sNSPX+EUERSg8ed1kyT936Idll26BwCAN5gebp599llNmjRJGRkZ6tevn2bPnq2QkBDNmTOn2fZvvPGG7r77bqWkpCg5OVl///vf5Xa7lZ2d3cqVn9rJ9rlpqdtGJSnQ365VuSVavueIV54TAAArMjXc1NXVaeXKlUpPT/ccs9vtSk9P19KlS0/rOaqqqlRfX68OHTo0e39tba3Kysqa3FqDZ4diL106oVNEkG4a2kWSNGvRDq88JwAAVmRquCkuLpbL5ZLT6Wxy3Ol0Kj8//7Se43e/+53i4+ObBKTvy8rKUmRkpOeWmJj4o+s+HSdbLfVj3HFBD9lt0pdbi7Qmr8RrzwsAgJWc1VveTp8+XXPnztWiRYsUFBTUbJvJkycrMzPT83NZWVmrBBxvTig+Jik2VNcP6aJ/r9qnJz7YqHfvGtns5n8AAN9wuQ0t231In20s0KHKOjW43Kp3uVXvMtTgduuiczrpVxd0599mk5kabmJjY+Xn56eCgoImxwsKChQXd/LN7STpmWee0fTp07Vw4UINGjTopO0cDoccDodX6j0Tvui5kaTfXtFHn2w4qNW5JfrPmgMaOyTBq89/Jhpcbn21vUgD4iPVKaL5cAkAZzu329CKvUf04boD+nh9voorak/a9psdh7SjsEJ/uGGg/OwEHLOYGm4CAwM1dOhQZWdna+zYsZLkmRx87733nvRxf/rTnzRt2jR9+umnSk1NbaVqz8zxCcXe/YidEUG65+JeevrTrcr6ZLMu6+dUqKP1T2Ndg1v3vrlKn20qUKCfXTcO7aJfX9hDSbGhrV4LALTEkco6bT5Ypk0Hy7T5YLk2HyzTjsIKNbjd8rfb5We3yd9uk8swPL+wSlJkcIDG9HeqT1yEAv1s8vezK8DProMl1frLwm2atyJP5bX1+su4FDn8vfsL7qnUNbhVUFaj/LIaHSytUXVdgwYkRCo5LqLdBS3Th6UyMzM1ceJEpaamavjw4ZoxY4YqKyuVkZEhSZowYYISEhKUlZUlSfrjH/+oKVOm6M0331RSUpJnbk5YWJjCwsJMex//zRfDUsfcfn53zV2eq7zD1Zq9eKf+5/I+Xn3+2gaXFm8t0ogeMYoMDjjh/pp6l+5+Y5W+2FIom02qc7n1Vk6u5i3P1ZUDO+uu0T1Pa/dlAGhNpdX1WrbrkL7deUjf7izWtoKKk7atc7ml43lG4UH+urxfnH4yuLNG9Yw96UasvTqF6YG5a/Tx+nyV16zQ7F8O9ekvoMUVtZqxcJs+3Vig4opaNbdTSJjDX0O6RmlYUgcN795Bw5M6yG7xsGN6uBk3bpyKioo0ZcoU5efnKyUlRQsWLPBMMs7NzZXdfvwv0axZs1RXV6ebbrqpyfNMnTpVjz/+eGuWfkreXi31fUEBfnrkqn668/WVeuGrXfpZaqISO4R47fkfeW+D3lm5TzGhgfrtFX3006GJnv8RaupduuO1lfpqW5Ec/na9NCFVwYF+mrVop77YUqiP1h3UR+sO6qahXfS/YweccFV0AGht2wvK9bt/r9OavBL99ybv3WJClBwXrr6dI5QcF6HkuHCFBPqpwW3I5TY8/03sEHxavTBXDuys8KAA3fHaCn29vVi/fHmZXrl1mKJCAr36nmrqXXrlmz2a+eUOVdQ2eI4H+tkVd/T6hf5+Nq3NK1VFbYO+3l6sr7cXS5J6dAzVry/sYeq0Bl+zGe1sR7iysjJFRkaqtLRUERERPnkNt9tQj99/LEla+Wi6YsK8P+fHMAyN//syfbvzkK4aGKf/Gz/UK8+78UCpfvK3JU3S/6AukXr82v5KjgvXr/6xQt/uPKTgAD+9PDFVI3vFetptPlimFxbv1PtrD8htSEO6RumFW4aqUzjzcQCYo8Hl1rXPf6NNBxu3AenRMVQje8ZoVM9YjegRow6h3g0dx6zOPaJbX1mu0up6dQx3aGxKvK5LSVD/+IgfNdnYMAx9vD5f0xdsVt7haknSwIRI/WZMH/WPj1CH0MAmz+9yG9qSX6YVe45oxd4jWrS1UOU1jWHIGeFQZHCAthVU6P5LeinTy6MA3nYm39+EGx+orG1Q/6mfSpI2PTnG6/NujtmSX6ar/vq13Ib01qTzlNYz5kc/5y0vL9PX24t19cDOGtI1SjMWbvf8VtAlOlj7jlQrNNBPr2QM1/Duze8ttGR7se5+Y6XKahrUOTJIL01IZZgKgCle/Wa3Hv9gkyKC/PXBfeerW0zrzQvcml+ujFdydKC0xnOsV6cwjU2JV1rPGAUH+CsowK6gAD85/O2qqG3QruJK7S6q1K7iCu0urlRhWa1qGlyqrnOrtt6l6nqXGo52PzkjHPrtmGRdPyThtIeZymvq9VZOrl5eslsFZccnRhNuznKtEW6Kyms1bNpCSdKuP1zl07HNx+Zv0Gvf7VVyXLg+uv+CHzVpbPG2Ik2ck6MAP5u++J+LlNghRIXlNXp6wVa9vXKfJCnc4a9Xbxumod2aDzbH7C6u1O3/WK5dRZUKCrDrzz9N0dWDOre4NgA4U0XltbrkmUUqr23QU2MH6Jajl7FpTbUNLi3aWqT/rNmvhZsLVdfg/tHPGRzgpzsu7KFfj+7R4l+eaxtc+s/qA3rhq53aWVSpGeNS2vwwFeHmFFoj3OQeqtKFT3+p4AA/bX7qCp+8xjFHKut00TOLVFpdrxduGaox/U+9hP5kXG5DVz/3tbbkl+v287vrsZ/0a3L/6twjenfVfo0blnjavTCl1fW6/63VWrytSJI0onsHxUcFq1OEQ87wIDkjgpTW03fdwgDat8x5a/Tu6v0amBCp+feMMn3FUFlNvRZsyNcHaw9oz6FK1dS7VVPvUm29W3UutwL97eoeE6oeHUPVPbbxlhAVrKBAPwX5+yk40E9BAXZFhwR6bT6j222osLxWcZFtf/rAmXx/mz6h2Iqq6huHcXyxUuq/RYcG6uZhiXrhq116b9X+Foebd1ft05b8ckUE+eu+S3qdcP+QrtEa0jX6jJ4zMjhAc24dpqyPN+vvS3Zr2e7DJ7TpEBqoF24ZqmFJp+4JAoAzsWzXIb27er9sNumpsQNMDzZS40WQf5aaqJ+lnriRrMttyCa1+iomu912VgSbM0W48QFfbeB3MmOHJOiFr3bpiy2FKq2qV2TIicu3T6Wm3qU/f7ZNknTPxb28Oqvfz27Toz/pp7FDErS9sFyFZbUqKKtVQXmNNuwv1d5DVfrFS9/pD9cP1E+b+R8ewNmvorZBH687qHdW7dOuokqFOfwUFuSvMIe/whwBigoJUFxEkJyRQYqLaFzpk9ghpNmtKE5Hvcutx/6zQZJ087CuSkmM8uK78Y22EL6shHDjAzVeviL4D2lcwhiuLfnl+mj9Qf1iRNczevzLS3Yrv6xGCVHBmjgyySc1DkiIPGE4q6quQf/zr7X6ZEO+fvPOOu0oqtBvxyTzPzlgAW63oe92H9I7K/bpkw35nr2/JKn45NvLePjbbfrVBT10/6W9znheyT++3aNtBRWKDgnQb8e07Umy8A3CjQ8c77lpvY/3+iEJyvpki95bve+Mws2hilrNWrRTkvTQmHNadV+akEB/zfzFufrLwm362xc79MLiXdpZWKm/3pxiyq7LAFqurKZe6/JKtTr3iFbnlWh17hEdqar33N+jY6huGtpFF/TqqNoGl8prG1RR06CK2gYdrqxTwdFddQvKanSgpEbFFbWavXinPlh7QI9f21+X9XOe4tWP23ekSn/5vLEn+uErkxXNnL52iW8QH6g6tjtxKwaF61ISNH3BFi3fc0R5h6tOe1O/57Ibl3r3j4/QdYNbf6a83W7T/1zeR706hek376zTws0F+unspXr1tmHsjwP4QGF5jTbuL1NUSID6do446S80xRW12nywTBU1DbLZbLLbJLvNJrtdKqmq174j1dp3pEr7S6qVd7haeUeqTtgdN9zhr58MjtdPU7toSGLUGe3vsnBTgaa+v1H7S6o16Z8rdFk/px6/tr8SooKbbZ97qEovfb1L/1qRp9oGt4Z0jdJPhzLU3V4Rbnyg+tjuxK00LCVJcZFBGtUzVkt2FGv+6v2679LeP/iYHYXlen1ZriTpkav6mrod93UpCeraIUST/rlCmw6W6aZZS/Xa7cNbdU8KwGqq61zaXliu1bklWpV7RKtyj3g2fpMa53n07hSm/vGR6h8fodLqem08UKqNB8p08Ht7s5yuxA7BGpIYrSFdozSka7T6dY446WUKfkh6P6dG9orRc9k79Pevd+nzTQVavLVIyZ3D1a9zhPrHR6hffIT87Ha9vGS3Plp3wLP78KAukXr2ZymWv8QATo5w4wOtPaH4mLFDErRkR7HeW7Nf917S6wd/S5r20Wa53IbS+zqb7DRsliFdo/Xvu0bqlpdzlHu4SjfOWqp/3DZM/ePZALCuwa2ZX+5Qdb1LCVHB6hIdrIToYCVEBSs8qGWTLmEdhmEo93CVlu0+rG355dpRVKEdhRXad6T6hLY2m9SzY5iOVNbpUGWdtuSXa0t+uf696sR23WNCFRvmkNswjt4kt2EoPMhfXaJClBB99O9iVLB6dAxTx3Dv7sYeEuivh69M1g3nJujR+RuUs/uw1u0r1bp9pc22v/CcjrpzdA+l9Yj5UbsA4+xHuPGBY+GmNYelJOmKAXF6dP567Sqq1Lp9pRp8ihUCX20r0pdbi+Rvt+n3VyW3XpE/oFtMqN65K00T5yzX5oNluvmF7/TihFSv7L58tjIMQw//e53eXb2/2fsTooKV1jNGaT1ilNYzRvEn6bbH2aGqruG0JtAWltdo6c5D+mZHsb7ZcUj7S04MMpIUHRKggV2iNLRrtM7tFqXBiVGKCAqQYRjKL6vRhv1lWr+/VJsPlik8yF8Dj07+79s5QmFtZO7bOc5wzbvjPO09VKVNB8u08UCpNh1ovJr3oYo6XTWws349uge/CMGjbfzNtZjqVl4tdUyYo/Gqte+vPaD3Vu8/abhpcLk17aPNkqRb0rqpR8e2czV1SeoUHqR5vz5Pk/6xQst2H9bEV3L01HX9dc3geJ9dysLXahtcWripUIu2FqpbTIgu6N1RAxIiT2tl2IyF2/Xu6v3ys9s0bliiDlfUaV9JlfYfqdaRqnrtL6nWOyv36Z2ju0gnxYTo/N6xSu/rVFrPmNO62B/MV1ZTr8n/Xq+P1h9Ucly4Lu3bSZf2dWpwlyj52W1yuQ2tySvRoq2F+mJLoTYeKGvyeH+7TUO6Rql/fKR6O8PUq2OYenUKO+m17Ww2mzpHBqtzZPBpT9Y1k81mU1JsqJJiQ3XVwOO7nbvdBsNPOAE7FPvAkx9s0pxvduvO0T318JWt2yvy5dZCZbyyXDGhgfru95cqwO/E8e43lu3VI+9tUGRwgBb/5iKvX63WW2rqXbr/rdX6bFOBJMnhb9f5vWJ1WT+nLu3r9HoXuC9syS/TvOV5mr96f5OVI1Ljb9SjesXqwt4dNWZAXLN7eryzcp8eenutJCnrhoH6+fCmK+HKa+q1KrdES3ce0tKdxVq/v7TJVY9DA/104Tkdld7XqUuSO7FypI3adKBMd7+xUnsOVZ1wX0xooAZ1idSavJIT/g716xyhUb1iNLJXrIYndWCVISyNyy+cQmuEm8nvrtdbObnKvOwc3X8aE3u9qcHl1nlZ2SquqNOcW1N1SXLT38jKaup18dOLdKiyTlOv6aeMUd1btb4z1eBya+aXO/XvVfuUe/j4P/w2mzSqZ6x+d0WyBnZpe13R6/eV6tH567X2e3MDnBEOXT0wXvuOVOnbnYc8FySVGleVZIxK0m3nd/eEzW93FGvCnBw1uA3ddVFP/e6KHw7KZTX1ytl1WF9sLdTCTQUqLD9+YTw/u00je8boygGdNaa/0ydXq7cSt9vQnkOVWr+/cY7H+v2ligwO0DM/HdzizeWa86/leXrsPxtU2+BWQlSw/njjIBVV1Ch7c6EWbyvyXMFZksKD/HXhOR11cZ9OGn1Ox7Mi4APeQrg5hdYINw/OXa35aw7o0av76lcX9PDJa5zKEx9s1Cvf7NFPBnXW8784t8l9WZ9s1guLd6lHx1B9+uCFzfbstEWGYWhbQYU+35SvzzcVNAkNNwxJ0G+u6KPOkW1jrslX24p05+srVVXnUoCfTel9nfpZaqIu6B0r/6Ofd73LrTV5Jfp6W5E+2ZCv7YWNu5qFOfx168gkje7TUbe9ulzlNQ36yaDOeu7mIWfc9e52G9pwoFQLNxXos00F2pJf7rnPz27TeT066LqUBI1NSWjxipYzVVHboBcX79S/V+1XTb1LLsOQy23IMCS7Tbq0r1MZo5I0qEtUq9Tz38pq6vXZxgJ9tO6AVuw5ovLvBdBjhnSN0mu3j/jR81Gq61x67D8bPMOJF/fpqGd/ltKkd63e5daKPUe08UCpBiZE6txu0WfN/7OAtxFuTqE1ws0d/1yhzzYV6H/HDtAvTbgK7fp9pbrm+SVy+Nv1mzF9FBEUoLAgf9kkPTB3jepcbr08MVWX9m374+wnk3e4caOuY5NsgwLsuuOCHvr16J6mds3PX71fD729Vg1uQ+f3itWMm1MU+wM9JG63oU835uuv2dubBBBJSu0Wrdd/NcIrmyvuLq7UJxsO6uP1B7Vh//H5Gl2ig3X/Jb11/bkJPvvibHC59fbKffrzZ9tUXFH7g+3P7RqlW0d115UD4n6wpvKaen25tUj7jlTpUEWdDlXU6lBlnQ5V1Kmm3qU6l1v1LrfqXYbqXW51CA1U707hOscZpnOc4erVKUy7iiv1wdoDWry1SHWu41dtdvjb1T8+QoO6RKlHx1A9+/k2lVTVa3hSB71627AWzQEzDEMLNxfqyQ83Ku9wtew26X8u76O7Rvdk7ghwCoSbU2iNcHPLy8v09fZiPfuzwbrh3C4+eY1TMQxDl/3lK+0obH6P8/N7xeq124dbYqnkun0l+t8PNytnT+NFOTuGO/Sby/voxqFdfvRlHI7t+RHm8FdUcKCiQgMU7vA/6ef20le7NO3jxona1w6O1zM/HXxGPSJut6HPNhXor9nbtflgmZJiQvTu3aN8ctX03ENV+nD9Ab3yzR4VHR266hYTovsv6a3rUuI9PUzNyTtcpaxPNitn9xF1iwlRn7hw9Y0LV5+4CHWPDZX/f33ua/eVKOvjLdpa0BjckmJC9NCYPjrHGX58YzibTcUVtXpzWa4+WHdA9a7Gf5acEQ5d2tep4UkdlJoUrS7RjZtT1rvc+mpbkd5bvV+fbypQbYNb3tKrU5iuGRSvy/o5dY4zrMlnsX5fqX7x0ncqr23QqF4xennisDMKnruLK/XEBxu1aGuRJCkuIkjPjhuskT3N34oBaOsIN6fQGuHmxlnfauXeI5r9y3N1xYDOP/wAH1i3r0Tzluep/Oj25hU1DSqvbZC/3aYZN6eoZxtbIfVjGEZjz0fWJ1u09+iEzOS4cD32k34adYb79xiGoZzdhzV3eZ4+Xn/whC9NP7tN0SGB6hYTou6xoZ7b8j2H9co3eyRJt43qrkevbvmmiG63oTX7StSzY5hX53Y0p7rOpTeW7dWsRTt1qLJOUmPI+Vlqoq4fktBkWXlVXYP+78udevHrXaprQZiIDA7QA5f21i/P63bK0FdYXqO3luXp9WV7PcHrmPjIIPWLj9Sq3CM6fLReSerZMVRDukYrJjRQMWGBigl1qENYoEID/RXgZ1OAn10Of7v87Dbll9Voe0GFthaUa3tBubYXVig6JFBXDYzTNYPj1ccZfsrgv3LvEU14eZkq61y6uE9Hzb5lqGdFWkVtg/YfqdahylpPaLPbGlf6ZG8u0N+/3q06l1sBfjZNuqCH7rm4F5OAgdNEuDmF1gg3V/71a20+WKZ/3DZco8/p6JPXwIlqG1x6belePZe9XWVHJ2FemtxJ91zSS53CHQpz+CvU4e8Z5qhtcKm0ql4l1fU6UlmnNXmNgXBXcaXnOROiguU2DJVU1Te58N/JTL4yWXdc2OOs6xWrqmvQP77dqxe+2qmSoytybLbGXr4bj/Y+Tv9ki/LLGnetHdUrRvdc1EtFFbXakl+urUdvze21EhRg1/gR3XTfJWd2xfm6BrcWbyvSsl2HtHzvEW3YXyrX95aCxYY5dF1KvK4fkqD+8RGt+pkv23VIE1/JUU29W307R8jfbtO+I1UnrGZqzuhzOmrqNf3a3BYMQFtHuDmF1gg3Fz39pfYcqtLbd6ZpWFIHn7wGTu5IZZ3+mr1dr3+3Vw3uE/96B/rZZbdLNfXN9z6EBPrp2sHxunl4Vw3uEun50qypd6m0ul5F5bXac6hSu4sqtftQpXYXV6qytkF3X9RLY4e0/vW5vKmytkEfrz+od1bu07Ldh0+4v0t0sB69up/G9Hc2GyZczXzeNskrc0mq6hq0OrdEGw+U6hxnuM7vFXvK4TNfW7K9WLf9Y/kJvVhRIQGKOTqUaBiSocZdfaOCA3TvJb2V3rfTWRd+gbaAcHMKrRFuhk9bqMLyWn143/kakND2lim3F7uKKvT0p1v13a5DqqxzNTuUYrdJUSGBigoOkDMiSNemxOuawfFtZmdWM+UeqtK/V+3Tv1ftU1l1vSZd0EOTLuzRqleOb+u2F5Tru92H1TkiSF06cDkMwJcIN6fQGuFm4OOfqrymQV/8z2i6ntuQepdbVbUuVdY1qMFlKDKkcYIwK1ROzTAMehoAmO5Mvr/59dQHjl9+gY+3LQnwsysyxK7IEH6zPhMEGwBnG3aD8rK6Brdnnkcw3fcAALQ6wo2XHeu1kaTgVr5wJgAAINx4XVV94xJkf7ut1ba0BwAAx/Ht62VVR3tu6LUBAMAchBsvOz6ZmHADAIAZCDdeVsVKKQAATEW48bJjW/SzUgoAAHMQbrysuq5xQjHDUgAAmINw42VMKAYAwFyEGy/zhBuGpQAAMAXhxstYLQUAgLkIN152fFiK1VIAAJiBcONlx3YopucGAABzEG68jGEpAADMRbjxsmpWSwEAYCrCjZdVHd3EL4TVUgAAmIJw42X03AAAYC7CjZdVHd2hmNVSAACYg3DjZZ4JxQxLAQBgCsKNl1WxWgoAAFMRbryMa0sBAGAuwo2XVR9bLcWcGwAATEG48TI28QMAwFyEGy9yuw1Pzw3DUgAAmINw40U1DS7Pn4NZLQUAgCkIN150bDKxRLgBAMAshBsvOjbfJijALrvdZnI1AAC0T4QbLzq+xw0rpQAAMAvhxos8l15gSAoAANMQbryIZeAAAJiPcONFxzfwI9wAAGAWwo0XVXkmFBNuAAAwC+HGixiWAgDAfIQbLzo2oZjVUgAAmIdw40VVXHoBAADTEW68iGEpAADMR7jxomMTium5AQDAPIQbL/LsUBzAnBsAAMxCuPGiGva5AQDAdIQbLzq2WiqIcAMAgGlMDzczZ85UUlKSgoKCNGLECOXk5Jy07caNG3XjjTcqKSlJNptNM2bMaL1CT8PxYSnCDQAAZjE13MybN0+ZmZmaOnWqVq1apcGDB2vMmDEqLCxstn1VVZV69Oih6dOnKy4urpWr/WGslgIAwHymhptnn31WkyZNUkZGhvr166fZs2crJCREc+bMabb9sGHD9PTTT+vmm2+Ww+Fo5Wp/GKulAAAwn2nhpq6uTitXrlR6evrxYux2paena+nSpV57ndraWpWVlTW5+crxC2eyWgoAALOYFm6Ki4vlcrnkdDqbHHc6ncrPz/fa62RlZSkyMtJzS0xM9Npz/7fjl1+g5wYAALOYPqHY1yZPnqzS0lLPLS8vz2evxbAUAADmM238JDY2Vn5+fiooKGhyvKCgwKuThR0OR6vNz2GfGwAAzGdaz01gYKCGDh2q7OxszzG3263s7GylpaWZVVaL1bvcqncZkqRgloIDAGAaU2e+ZmZmauLEiUpNTdXw4cM1Y8YMVVZWKiMjQ5I0YcIEJSQkKCsrS1LjJORNmzZ5/rx//36tWbNGYWFh6tWrl2nvQzo+JCUxLAUAgJlMDTfjxo1TUVGRpkyZovz8fKWkpGjBggWeSca5ubmy2493Lh04cEBDhgzx/PzMM8/omWee0ejRo7Vo0aLWLr+JY3vc+NltCvSz/FQmAADaLJthGIbZRbSmsrIyRUZGqrS0VBEREV573l1FFbrkz4sV7vDX+ifGeO15AQDAmX1/08XgJayUAgCgbSDceEk1K6UAAGgT2ErXS3p3CtOcW1PlZycvAgBgJsKNl0SFBOqSZOcPNwQAAD5FNwMAALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALCUNhFuZs6cqaSkJAUFBWnEiBHKyck5Zfu3335bycnJCgoK0sCBA/Xxxx+3UqUAAKCtMz3czJs3T5mZmZo6dapWrVqlwYMHa8yYMSosLGy2/bfffquf//znuv3227V69WqNHTtWY8eO1YYNG1q5cgAA0BbZDMMwzCxgxIgRGjZsmJ5//nlJktvtVmJiou677z49/PDDJ7QfN26cKisr9eGHH3qOnXfeeUpJSdHs2bN/8PXKysoUGRmp0tJSRUREeO+NAAAAnzmT72//VqqpWXV1dVq5cqUmT57sOWa325Wenq6lS5c2+5ilS5cqMzOzybExY8Zo/vz5zbavra1VbW2t5+fS0lJJjR8SAAA4Oxz73j6dPhlTw01xcbFcLpecTmeT406nU1u2bGn2Mfn5+c22z8/Pb7Z9VlaWnnjiiROOJyYmtrBqAABglvLyckVGRp6yjanhpjVMnjy5SU+P2+3W4cOHFRMTI5vN5tXXKisrU2JiovLy8hjyaiM4J20P56Tt4Zy0TZyXpgzDUHl5ueLj43+wranhJjY2Vn5+fiooKGhyvKCgQHFxcc0+Ji4u7ozaOxwOORyOJseioqJaXvRpiIiI4C9iG8M5aXs4J20P56Rt4rwc90M9NseYuloqMDBQQ4cOVXZ2tueY2+1Wdna20tLSmn1MWlpak/aS9Pnnn5+0PQAAaF9MH5bKzMzUxIkTlZqaquHDh2vGjBmqrKxURkaGJGnChAlKSEhQVlaWJOmBBx7Q6NGj9ec//1lXX3215s6dqxUrVujFF180820AAIA2wvRwM27cOBUVFWnKlCnKz89XSkqKFixY4Jk0nJubK7v9eAfTyJEj9eabb+rRRx/V73//e/Xu3Vvz58/XgAEDzHoLHg6HQ1OnTj1hGAzm4Zy0PZyTtodz0jZxXlrO9H1uAAAAvMn0HYoBAAC8iXADAAAshXADAAAshXADAAAshXDjJTNnzlRSUpKCgoI0YsQI5eTkmF1Su5GVlaVhw4YpPDxcnTp10tixY7V169YmbWpqanTPPfcoJiZGYWFhuvHGG0/YDBK+M336dNlsNj344IOeY5wTc+zfv1+//OUvFRMTo+DgYA0cOFArVqzw3G8YhqZMmaLOnTsrODhY6enp2r59u4kVW5vL5dJjjz2m7t27Kzg4WD179tRTTz3V5PpJnJMWMPCjzZ071wgMDDTmzJljbNy40Zg0aZIRFRVlFBQUmF1auzBmzBjjlVdeMTZs2GCsWbPGuOqqq4yuXbsaFRUVnjZ33nmnkZiYaGRnZxsrVqwwzjvvPGPkyJEmVt1+5OTkGElJScagQYOMBx54wHOcc9L6Dh8+bHTr1s249dZbjWXLlhm7du0yPv30U2PHjh2eNtOnTzciIyON+fPnG2vXrjWuvfZao3v37kZ1dbWJlVvXtGnTjJiYGOPDDz80du/ebbz99ttGWFiY8de//tXThnNy5gg3XjB8+HDjnnvu8fzscrmM+Ph4Iysry8Sq2q/CwkJDkrF48WLDMAyjpKTECAgIMN5++21Pm82bNxuSjKVLl5pVZrtQXl5u9O7d2/j888+N0aNHe8IN58Qcv/vd74zzzz//pPe73W4jLi7OePrppz3HSkpKDIfDYbz11lutUWK7c/XVVxu33XZbk2M33HCDMX78eMMwOCctxbDUj1RXV6eVK1cqPT3dc8xutys9PV1Lly41sbL2q7S0VJLUoUMHSdLKlStVX1/f5BwlJyera9eunCMfu+eee3T11Vc3+ewlzolZ3n//faWmpuqnP/2pOnXqpCFDhuill17y3L97927l5+c3OS+RkZEaMWIE58VHRo4cqezsbG3btk2StHbtWi1ZskRXXnmlJM5JS5m+Q/HZrri4WC6Xy7Oj8jFOp1Nbtmwxqar2y+1268EHH9SoUaM8u1bn5+crMDDwhAumOp1O5efnm1Bl+zB37lytWrVKy5cvP+E+zok5du3apVmzZikzM1O///3vtXz5ct1///0KDAzUxIkTPZ99c/+ecV584+GHH1ZZWZmSk5Pl5+cnl8uladOmafz48ZLEOWkhwg0s5Z577tGGDRu0ZMkSs0tp1/Ly8vTAAw/o888/V1BQkNnl4Ci3263U1FT94Q9/kCQNGTJEGzZs0OzZszVx4kSTq2uf/vWvf+mNN97Qm2++qf79+2vNmjV68MEHFR8fzzn5ERiW+pFiY2Pl5+d3wiqPgoICxcXFmVRV+3Tvvffqww8/1JdffqkuXbp4jsfFxamurk4lJSVN2nOOfGflypUqLCzUueeeK39/f/n7+2vx4sV67rnn5O/vL6fTyTkxQefOndWvX78mx/r27avc3FxJ8nz2/HvWen7zm9/o4Ycf1s0336yBAwfqlltu0f/7f//Pc7FozknLEG5+pMDAQA0dOlTZ2dmeY263W9nZ2UpLSzOxsvbDMAzde++9eu+99/TFF1+oe/fuTe4fOnSoAgICmpyjrVu3Kjc3l3PkI5deeqnWr1+vNWvWeG6pqakaP36858+ck9Y3atSoE7ZJ2LZtm7p16yZJ6t69u+Li4pqcl7KyMi1btozz4iNVVVVNLg4tSX5+fnK73ZI4Jy1m9oxmK5g7d67hcDiMV1991di0aZNxxx13GFFRUUZ+fr7ZpbULd911lxEZGWksWrTIOHjwoOdWVVXlaXPnnXcaXbt2Nb744gtjxYoVRlpampGWlmZi1e3P91dLGQbnxAw5OTmGv7+/MW3aNGP79u3GG2+8YYSEhBivv/66p8306dONqKgo4z//+Y+xbt0647rrrmPZsQ9NnDjRSEhI8CwFf/fdd43Y2Fjjt7/9racN5+TMEW685G9/+5vRtWtXIzAw0Bg+fLjx3XffmV1SuyGp2dsrr7ziaVNdXW3cfffdRnR0tBESEmJcf/31xsGDB80ruh3673DDOTHHBx98YAwYMMBwOBxGcnKy8eKLLza53+12G4899pjhdDoNh8NhXHrppcbWrVtNqtb6ysrKjAceeMDo2rWrERQUZPTo0cN45JFHjNraWk8bzsmZsxnG97ZBBAAAOMsx5wYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QZAu2ez2TR//nyzywDgJYQbAKa69dZbZbPZTrhdccUVZpcG4Czlb3YBAHDFFVfolVdeaXLM4XCYVA2Asx09NwBM53A4FBcX1+QWHR0tqXHIaNasWbryyisVHBysHj166J133mny+PXr1+uSSy5RcHCwYmJidMcdd6iioqJJmzlz5qh///5yOBzq3Lmz7r333ib3FxcX6/rrr1dISIh69+6t999/37dvGoDPEG4AtHmPPfaYbrzxRq1du1bjx4/XzTffrM2bN0uSKisrNWbMGEVHR2v58uV6++23tXDhwibhZdasWbrnnnt0xx13aP369Xr//ffVq1evJq/xxBNP6Gc/+5nWrVunq666SuPHj9fhw4db9X0C8BKzr9wJoH2bOHGi4efnZ4SGhja5TZs2zTCMxqu+33nnnU0eM2LECOOuu+4yDMMwXnzxRSM6OtqoqKjw3P/RRx8ZdrvdyM/PNwzDMOLj441HHnnkpDVIMh599FHPzxUVFYYk45NPPvHa+wTQephzA8B0F198sWbNmtXkWIcOHTx/TktLa3JfWlqa1qxZI0navHmzBg8erNDQUM/9o0aNktvt1tatW2Wz2XTgwAFdeumlp6xh0KBBnj+HhoYqIiJChYWFLX1LAExEuAFgutDQ0BOGibwlODj4tNoFBAQ0+dlms8ntdvuiJAA+xpwbAG3ed999d8LPffv2lST17dtXa9euVWVlpef+b775Rna7XX369FF4eLiSkpKUnZ3dqjUDMA89NwBMV1tbq/z8/CbH/P39FRsbK0l6++23lZqaqvPPP19vvPGGcnJy9PLLL0uSxo8fr6lTp2rixIl6/PHHVVRUpPvuu0+33HKLnE6nJOnxxx/XnXfeqU6dOunKK69UeXm5vvnmG913332t+0YBtArCDQDTLViwQJ07d25yrE+fPtqyZYukxpVMc+fO1d13363OnTvrrbfeUr9+/SRJISEh+vTTT/XAAw9o2LBhCgkJ0Y033qhnn33W81wTJ05UTU2N/vKXv+ihhx5SbGysbrrpptZ7gwBalc0wDMPsIgDgZGw2m9577z2NHTvW7FIAnCWYcwMAACyFcAMAACyFOTcA2jRGzgGcKXpuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApRBuAACApfx/zXWbLshQ/j0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.linspace(0, len(train_losses),len(train_losses)), train_losses, label = 'RMSprop')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "ax = plt.gca()\n",
        "ax.set_ylim([0, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro8STR5kl_Yi",
        "outputId": "fa142d90-5d50-438e-ccca-6daffcdf051c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nan\n",
            "[8.58982280e-02 6.95612393e-02 1.02768791e-01 1.02664955e-01\n",
            " 1.19539800e-01 1.15663476e-01 1.31112265e-01 1.24597020e-01\n",
            " 1.40771305e-01 1.36595406e-01 1.29154758e-01 1.51666604e-01\n",
            " 1.52710115e-01 1.53938802e-01 1.54441986e-01 1.55807642e-01\n",
            " 1.52623307e-01 1.50021800e-01 1.47281504e-01 1.52569625e-01\n",
            " 1.49364246e-01 1.48390465e-01 1.48238878e-01 1.50535801e-01\n",
            " 1.48626199e-01 1.48082500e-01 1.44583934e-01 1.52529952e-01\n",
            " 1.67928771e-01 1.66502399e-01 1.66303685e-01 1.66914885e-01\n",
            " 1.67090340e-01 1.67770412e-01 1.72242173e-01 1.72321353e-01\n",
            " 1.71029516e-01 1.71189451e-01 1.70449530e-01 1.69313207e-01\n",
            " 1.68475608e-01 1.72342226e-01 1.74028272e-01 1.74338251e-01\n",
            " 1.78223300e-01 1.76108248e-01 1.74943384e-01 1.74029426e-01\n",
            " 1.76235754e-01 1.77710101e-01 1.78588339e-01 1.78993888e-01\n",
            " 1.80234461e-01 1.80189164e-01 1.78867643e-01 1.72353006e-01\n",
            " 1.71818345e-01 1.73328489e-01 1.65864319e-01 1.64982285e-01\n",
            " 1.66713314e-01 1.68559876e-01 1.71505372e-01 1.71930676e-01\n",
            " 1.73442867e-01 1.73392002e-01 1.71469872e-01 1.79403705e-01\n",
            " 1.79448099e-01 1.80137014e-01 1.80787785e-01 1.80460072e-01\n",
            " 1.82941786e-01 1.87033240e-01 3.74730037e-01 1.15564221e+00\n",
            " 1.20496835e+06 1.25172361e+29 2.17962064e+29            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan\n",
            "            nan            nan            nan            nan]\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(train_losses))\n",
        "print(train_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqC5rLQYmBE-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}